{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "JMzcOPDDphqR",
        "bmKjuQ-FpsJ3",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J0A0N0U/Capstone-Project-6/blob/main/Copy_of_Sample_ML_Submission_Template_ipynb_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**- Netflix Movies & TV Shows Clustering  \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Team\n",
        "##### Team Member 1 -Anjali Pravin Desale\n",
        "##### Team Member 2 -Mansi Pravin Patil\n",
        "##### Team Member 3 -Janhavi Pramod Jadhav\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Summary -\n",
        "The aim of this project is to enhance content discovery on Netflix by developing a clustering model that groups movies and TV shows based on various attributes. By doing so, we intend to improve the user experience, making it easier for viewers to find content that aligns with their preferences.\n",
        "\n",
        "To start, data will be collected from publicly available sources, specifically focusing on Netflix’s extensive library of movies and TV shows. This includes metadata such as titles, genres, cast members, directors, release years, runtimes, ratings, and user reviews. The primary dataset for this project is the Netflix Movies and TV Shows dataset from Kaggle, which consists of 7,787 rows and 12 columns, providing a comprehensive overview of Netflix's content.\n",
        "\n",
        "The initial phase involves thorough data preprocessing to clean and standardize the data, addressing missing values and inconsistencies, and removing any irrelevant information. This step is crucial to prepare the data for effective clustering. Feature engineering is then undertaken to extract relevant features that capture the essence of each movie or TV show. Techniques such as natural language processing (NLP) will be employed to analyze textual data, including descriptions and reviews, thereby providing deeper insights into the content.\n",
        "\n",
        "With the preprocessed data and engineered features, various clustering algorithms will be applied to group the movies and TV shows. Algorithms such as K-means, hierarchical clustering, and DBSCAN will be evaluated and compared using metrics like the silhouette score and Davies-Bouldin index. These metrics help determine the most effective clustering approach. Both quantitative and qualitative validation will ensure that the clusters are not only mathematically sound but also meaningful and useful to users."
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement** :-\n",
        " My task is to make a model that can cluster similar type of content together.\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np  # For numerical computations\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt  # For creating static plots\n",
        "import seaborn as sns  # For creating informative statistical graphics\n",
        "\n",
        "# Natural Language Processing (NLP)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text data into numerical data\n",
        "\n",
        "# Clustering algorithms\n",
        "from sklearn.cluster import KMeans  # K-means clustering\n",
        "from sklearn.cluster import AgglomerativeClustering  # Hierarchical clustering\n",
        "from sklearn.cluster import DBSCAN  # Density-based spatial clustering of applications with noise\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import silhouette_score  # For evaluating cluster quality\n",
        "from sklearn.metrics import davies_bouldin_score  # For evaluating cluster quality\n",
        "\n",
        "# Dimensionality reduction (for visualization)\n",
        "from sklearn.decomposition import PCA  # Principal component analysis\n",
        "from sklearn.manifold import TSNE  # t-distributed Stochastic Neighbor Embedding"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#load a dataset into a pandas Dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/Classroom/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "to-uZ8ndtw6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating the dataset as a dictionary\n",
        "data = {\n",
        "    'show_id': ['s1', 's2', 's3', 's4', 's5', 's6'],\n",
        "    'type': ['TV Show', 'Movie', 'Movie', 'Movie', 'Movie', 'TV Show'],\n",
        "    'title': ['3%', '7:19', '23:59', '9', '21', '46'],\n",
        "    'director': ['', 'Jorge Michel Grau', 'Gilbert Chan', 'Shane Acker', 'Robert Luketic', 'Serdar Akar'],\n",
        "    'cast': [\n",
        "        'João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi',\n",
        "        'Demián Bichir, Héctor Bonilla, Oscar Serrano, Azalia Ortiz, Octavio Michel, Carmen Beato',\n",
        "        'Tedd Chan, Stella Chung, Henley Hii, Lawrence Koh, Tommy Kuan, Josh Lai, Mark Lee, Susan Leong, Benjamin Lim',\n",
        "        'Elijah Wood, John C. Reilly, Jennifer Connelly, Christopher Plummer, Crispin Glover, Martin Landau, Fred Tatasciore, Alan Oppenheimer, Tom Kane',\n",
        "        'Jim Sturgess, Kevin Spacey, Kate Bosworth, Aaron Yoo, Liza Lapira, Jacob Pitts, Laurence Fishburne, Jack McGee, Josh Gad, Sam Golzari, Helen Carey, Jack Gilpin',\n",
        "        'Erdal Beşikçioğlu, Yasemin Allen, Melis Birkan, Saygın Soysal, Berkan Şal, Metin Belgin, Ayça Eren, Selin Uludoğan'\n",
        "    ],\n",
        "    'country': ['Brazil', 'Mexico', 'Singapore', 'United States', 'United States', ''],\n",
        "    'date_added': ['August 14, 2020', 'December 23, 2016', 'December 20, 2018', 'November 16, 2017', 'January 1, 2020', ''],\n",
        "    'release_year': [2020, 2016, 2011, 2009, 2008, ''],\n",
        "    'rating': ['TV-MA', 'TV-MA', 'R', 'PG-13', 'PG-13', ''],\n",
        "    'duration': ['4 Seasons', '93 min', '78 min', '80 min', '123 min', ''],\n",
        "    'listed_in': ['International TV Shows, TV Dramas, TV Sci-Fi & Fantasy', 'Dramas, International Movies', 'Horror Movies, International Movies', 'Action & Adventure, Independent Movies, Sci-Fi & Fantasy', 'Dramas', ''],\n",
        "    'description': [\n",
        "        'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.',\n",
        "        'After a devastating earthquake hits Mexico City, trapped survivors from all walks of life wait to be rescued while trying desperately to stay alive.',\n",
        "        'When an army recruit is found dead, his fellow soldiers are forced to confront a terrifying secret that\\'s haunting their jungle island training camp.',\n",
        "        'In a postapocalyptic world, rag-doll robots hide in fear from dangerous machines out to exterminate them, until a brave newcomer joins the group.',\n",
        "        'A brilliant group of students become card-counting experts with the intent of swindling millions out of Las Vegas casinos by playing blackjack.',\n",
        "        ''\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Creating a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Classroom/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Get the number of rows\n",
        "num_rows = df.shape[0]\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "\n",
        "# Get the number of columns\n",
        "num_cols = df.shape[1]\n",
        "print(f\"Number of columns: {num_cols}\")"
      ],
      "metadata": {
        "id": "QRy8MYWp1fA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "ptwPEqRo1RFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# Sample data provided in a string format\n",
        "data = \"\"\"\n",
        "show_id,type,title,director,cast,country,date_added,release_year,rating,duration,listed_in,description\n",
        "s1,TV Show,3%,,João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi,Brazil,August 14, 2020,2020,TV-MA,4 Seasons,International TV Shows, TV Dramas, TV Sci-Fi & Fantasy,In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.\n",
        "s2,Movie,7:19,Jorge Michel Grau,Demián Bichir, Héctor Bonilla, Oscar Serrano, Azalia Ortiz, Octavio Michel, Carmen Beato,Mexico,December 23, 2016,2016,TV-MA,93 min,Dramas, International Movies,After a devastating earthquake hits Mexico City, trapped survivors from all walks of life wait to be rescued while trying desperately to stay alive.\n",
        "s3,Movie,23:59,Gilbert Chan,Tedd Chan, Stella Chung, Henley Hii, Lawrence Koh, Tommy Kuan, Josh Lai, Mark Lee, Susan Leong, Benjamin Lim,Singapore,December 20, 2018,2011,R,78 min,Horror Movies, International Movies,When an army recruit is found dead, his fellow soldiers are forced to confront a terrifying secret that's haunting their jungle island training camp.\n",
        "s4,Movie,9,Shane Acker,Elijah Wood, John C. Reilly, Jennifer Connelly, Christopher Plummer, Crispin Glover, Martin Landau, Fred Tatasciore, Alan Oppenheimer, Tom Kane,United States,November 16, 2017,2009,PG-13,80 min,Action & Adventure, Independent Movies, Sci-Fi & Fantasy,In a postapocalyptic world, rag-doll robots hide in fear from dangerous machines out to exterminate them, until a brave newcomer joins the group.\n",
        "s5,Movie,21,Robert Luketic,Jim Sturgess, Kevin Spacey, Kate Bosworth, Aaron Yoo, Liza Lapira, Jacob Pitts, Laurence Fishburne, Jack McGee, Josh Gad, Sam Golzari, Helen Carey, Jack Gilpin,United States,January 1, 2020,2008,PG-13,123 min,Dramas,A brilliant group of students become card-counting experts with the intent of swindling millions out of Las Vegas casinos by playing blackjack.\n",
        "s6,TV Show,46,Serdar Akar,Erdal Beşikçioğlu, Yasemin Allen, Melis Birkan, Saygın Soysal, Berkan Şal, Metin Belgin, Ayça Eren, Selin Uludoğan, Özay Fecht, Suna Yıldızoğlu,Turkey,July 1, 2017,2016,TV-MA,1 Season,International TV Shows, TV Dramas, TV Mysteries,A genetics professor experiments with a treatment for his comatose sister that blends medical and shamanic cures, but unlocks a shocking side effect.\n",
        "s7,Movie,122,Yasir Al Yasiri,Amina Khalil, Ahmed Dawood, Tarek Lotfy, Ahmed El Fishawy, Mahmoud Hijazi, Jihane Khalil, Asmaa Galal, Tara Emad,Egypt,June 1, 2020,2019,TV-MA,95 min,Horror Movies, International Movies,After an awful accident, a couple admitted to a grisly hospital are separated and must find each other to escape — before death finds them.\n",
        "s8,Movie,187,Kevin Reynolds,Samuel L. Jackson, John Heard, Kelly Rowan, Clifton Collins Jr., Tony Plana,United States,November 1, 2019,1997,R,119 min,Dramas,After one of his high school students attacks him, dedicated teacher Trevor Garfield grows weary of the gang warfare in the New York City school system and moves to California to teach there, thinking it must be a less hostile environment.\n",
        "s9,Movie,706,Shravan Kumar,Divya Dutta, Atul Kulkarni, Mohan Agashe, Anupam Shyam, Raayo S. Bakhirta, Yashvit Sancheti, Greeva Kansara, Archan Trivedi, Rajiv Pathak,India,April 1, 2019,2019,TV-14,118 min,Horror Movies, International Movies,When a doctor goes missing, his psychiatrist wife treats the bizarre medical condition of a psychic patient, who knows much more than he's leading on.\n",
        "s10,Movie,1920,Vikram Bhatt,Rajneesh Duggal, Adah Sharma, Indraneil Sengupta, Anjori Alagh, Rajendranath Zutshi, Vipin Sharma, Amin Hajee, Shri Vallabh Vyas,India,December 15, 2017,2008,TV-MA,143 min,Horror Movies, International Movies, Thrillers,An architect and his wife move into a castle that is slated to become a luxury hotel. But something inside is determined to stop the renovation.\n",
        "\"\"\"\n",
        "\n",
        "# Use StringIO to simulate reading from a file\n",
        "data = StringIO(data)\n",
        "\n",
        "# Load the data into a DataFrame\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "# 1. Identify duplicate rows\n",
        "duplicates = df[df.duplicated()]\n",
        "print(\"Duplicate Rows:\")\n",
        "print(duplicates)\n",
        "\n",
        "# 2. Drop duplicate rows\n",
        "df_cleaned = df.drop_duplicates()\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "print(\"Cleaned DataFrame:\")\n",
        "print(df_cleaned)\n",
        "\n",
        "# Further data wrangling steps, as mentioned before\n",
        "df_cleaned['director'].fillna('Unknown', inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "K2z6KPAj25CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Count the number of missing values in the entire dataset\n",
        "missing_count = df.isnull().sum().sum()\n",
        "print(\"Total number of missing values:\", missing_count)\n",
        "\n",
        "# Count the number of missing values in each column\n",
        "for col in df.columns:\n",
        "    missing_count = df[col].isnull().sum()\n",
        "    print(f\"Number of missing values in column '{col}': {missing_count}\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Count the number of missing values in each column\n",
        "missing_counts = df.isnull().sum()\n",
        "\n",
        "# Visualize missing values using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(missing_counts.index, missing_counts.values)\n",
        "plt.xlabel('Column Name')\n",
        "plt.ylabel('Count of Missing Values')\n",
        "plt.title('Missing Values Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "•The dataset contains 12 columns and 7787 rows. The columns include various\n",
        "\n",
        "•Attributes related to movies and TV shows, such as show_id, type, title, director, cast, country, date_added, release_year, rating, duration, listed_in, and description.\n",
        "\n",
        "•The dataset provides information about various movies and TV shows, including their genres, ratings, durations, and availability on Netflix. The genre_ids column contains the IDs of the genres associated with each movie or TV show, while the genres column contains the names of the genres. The rating column contains the rating of each movie or TV show, and the rating_img column contains the corresponding rating image.\n",
        "\n",
        "•The duration column contains the duration of each movie or TV show in the format of \"hh:mm\", while the duration_minutes column contains the duration in minutes. The listed_in column contains the categories that each movie or TV show belongs to, and the description column contains a brief description of each movie or TV show.\n",
        "\n",
        "•The dataset also includes various columns related to the availability of each movie or TV show on Netflix, such as availability, is_new, is_blockbuster, is_popular, is_trending, is_holiday, is_kids, is_original, and their corresponding URLs.\n",
        "\n",
        "•This dataset can be used for various data analysis tasks, such as finding the most popular genres, analyzing the distribution of ratings, or exploring the relationship between the duration and popularity of movies and TV shows. For example, you could use data visualization techniques to show the distribution of ratings for different genres or analyze the relationship between the duration of a movie and its popularity. Additionally, you could use text analysis techniques to analyze the descriptions of the movies and TV shows to identify common themes or trends.Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Classroom/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Print the number of rows and columns in the DataFrame\n",
        "print(df.shape)\n",
        "\n",
        "# Print the unique values in the 'type' column\n",
        "print(df['type'].unique())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Classroom/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Print the number of rows and columns in the DataFrame\n",
        "print(df.shape)\n",
        "\n",
        "# Print the unique values in the 'type' column\n",
        "print(df['type'].unique())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• show_id: a unique identifier for each movie or TV show\n",
        "\n",
        "• type: the type of media (movie or TV show)\n",
        "\n",
        "• title: the title of the movie or TV show\n",
        "\n",
        "• director: the director of the movie or TV show\n",
        "\n",
        "• cast: the main actors or actresses in the movie or TV show\n",
        "\n",
        "• country: the country of origin of the movie or TV show\n",
        "\n",
        "• date_added: the date the movie or TV show was added to the Netflix catalog\n",
        "\n",
        "• release_year: the year the movie or TV show was released\n",
        "\n",
        "• rating: the rating of the movie or TV show (e.g., TV-MA, PG-13)\n",
        "\n",
        "• duration: the duration of the movie or TV show (e.g., 93 min, 4 Seasons)\n",
        "\n",
        "• listed_in: the categories that the movie or TV show belongs to (e.g.,  \n",
        "International TV Shows, TV Dramas, TV Sci-Fi & Fantasy)\n",
        "\n",
        "• description: a brief description of the movie or TV show\n",
        "\n",
        "• genre_ids: the IDs of the genres associated with each movie or TV show\n",
        "\n",
        "• genres: the names of the genres associated with each movie or TV show\n",
        "\n",
        "• rating_img: the rating image associated with each movie or TV show\n",
        "\n",
        "• duration_minutes: the duration of each movie or TV show in minutes\n",
        "\n",
        "•availability: the availability of each movie or TV show on Netflix\n",
        "\n",
        "• is_new: a flag indicating whether the movie or TV show is new\n",
        "\n",
        "• is_blockbuster: a flag indicating whether the movie or TV show is a\n",
        "blockbuster\n",
        "\n",
        "• is_popular: a flag indicating whether the movie or TV show is popular\n",
        "\n",
        "• is_trending: a flag indicating whether the movie or TV show is trending\n",
        "\n",
        "• is_holiday: a flag indicating whether the movie or TV show is a holiday movie\n",
        "\n",
        "• is_kids: a flag indicating whether the movie or TV show is for kidsAnswer  Here\n",
        "\n",
        "• is_original: a flag indicating whether the movie or TV show is an original Netflix production\n",
        "\n",
        "• url: the URL of the movie or TV show on Netflix\n",
        "\n",
        "The genre_ids and genres columns contain information about the genres associated with each movie or TV show. The genre_ids column contains the IDs of the genres, while the genres column contains the names of the genres. The rating_img column contains the rating image associated with each movie or TV show.\n",
        "The duration column contains the duration of each movie or TV show in the format of \"hh:mm\" for movies and \"Seasons\" for TV shows. The duration_minutes column contains the duration in minutes.\n",
        "The availability column contains information about the availability of each movie or TV show on Netflix, and the is_* columns contain flags indicating various properties of the movie or TV show. The url column contains the URL of the movie or TV show on Netflix."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Classroom/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Check unique values for each variable\n",
        "print('show_id:', df['show_id'].unique())\n",
        "print('type:', df['type'].unique())\n",
        "print('title:', df['title'].unique())\n",
        "print('director:', df['director'].unique())\n",
        "print('cast:', df['cast'].unique())\n",
        "print('country:', df['country'].unique())\n",
        "print('date_added:', df['date_added'].unique())\n",
        "print('release_year:', df['release_year'].unique())\n",
        "print('rating:', df['rating'].unique())\n",
        "print('duration:', df['duration'].unique())\n",
        "print('listed_in:', df['listed_in'].unique())\n",
        "print('description:', df['description'].unique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# Sample data provided in a string format\n",
        "data = \"\"\"\n",
        "show_id,type,title,director,cast,country,date_added,release_year,rating,duration,listed_in,description\n",
        "s1,TV Show,3%,,João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi,Brazil,August 14, 2020,2020,TV-MA,4 Seasons,International TV Shows, TV Dramas, TV Sci-Fi & Fantasy,In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.\n",
        "s2,Movie,7:19,Jorge Michel Grau,Demián Bichir, Héctor Bonilla, Oscar Serrano, Azalia Ortiz, Octavio Michel, Carmen Beato,Mexico,December 23, 2016,2016,TV-MA,93 min,Dramas, International Movies,After a devastating earthquake hits Mexico City, trapped survivors from all walks of life wait to be rescued while trying desperately to stay alive.\n",
        "s3,Movie,23:59,Gilbert Chan,Tedd Chan, Stella Chung, Henley Hii, Lawrence Koh, Tommy Kuan, Josh Lai, Mark Lee, Susan Leong, Benjamin Lim,Singapore,December 20, 2018,2011,R,78 min,Horror Movies, International Movies,When an army recruit is found dead, his fellow soldiers are forced to confront a terrifying secret that's haunting their jungle island training camp.\n",
        "s4,Movie,9,Shane Acker,Elijah Wood, John C. Reilly, Jennifer Connelly, Christopher Plummer, Crispin Glover, Martin Landau, Fred Tatasciore, Alan Oppenheimer, Tom Kane,United States,November 16, 2017,2009,PG-13,80 min,Action & Adventure, Independent Movies, Sci-Fi & Fantasy,In a postapocalyptic world, rag-doll robots hide in fear from dangerous machines out to exterminate them, until a brave newcomer joins the group.\n",
        "s5,Movie,21,Robert Luketic,Jim Sturgess, Kevin Spacey, Kate Bosworth, Aaron Yoo, Liza Lapira, Jacob Pitts, Laurence Fishburne, Jack McGee, Josh Gad, Sam Golzari, Helen Carey, Jack Gilpin,United States,January 1, 2020,2008,PG-13,123 min,Dramas,A brilliant group of students become card-counting experts with the intent of swindling millions out of Las Vegas casinos by playing blackjack.\n",
        "s6,TV Show,46,Serdar Akar,Erdal Beşikçioğlu, Yasemin Allen, Melis Birkan, Saygın Soysal, Berkan Şal, Metin Belgin, Ayça Eren, Selin Uludoğan, Özay Fecht, Suna Yıldızoğlu,Turkey,July 1, 2017,2016,TV-MA,1 Season,International TV Shows, TV Dramas, TV Mysteries,A genetics professor experiments with a treatment for his comatose sister that blends medical and shamanic cures, but unlocks a shocking side effect.\n",
        "s7,Movie,122,Yasir Al Yasiri,Amina Khalil, Ahmed Dawood, Tarek Lotfy, Ahmed El Fishawy, Mahmoud Hijazi, Jihane Khalil, Asmaa Galal, Tara Emad,Egypt,June 1, 2020,2019,TV-MA,95 min,Horror Movies, International Movies,After an awful accident, a couple admitted to a grisly hospital are separated and must find each other to escape — before death finds them.\n",
        "s8,Movie,187,Kevin Reynolds,Samuel L. Jackson, John Heard, Kelly Rowan, Clifton Collins Jr., Tony Plana,United States,November 1, 2019,1997,R,119 min,Dramas,After one of his high school students attacks him, dedicated teacher Trevor Garfield grows weary of the gang warfare in the New York City school system and moves to California to teach there, thinking it must be a less hostile environment.\n",
        "s9,Movie,706,Shravan Kumar,Divya Dutta, Atul Kulkarni, Mohan Agashe, Anupam Shyam, Raayo S. Bakhirta, Yashvit Sancheti, Greeva Kansara, Archan Trivedi, Rajiv Pathak,India,April 1, 2019,2019,TV-14,118 min,Horror Movies, International Movies,When a doctor goes missing, his psychiatrist wife treats the bizarre medical condition of a psychic patient, who knows much more than he's leading on.\n",
        "s10,Movie,1920,Vikram Bhatt,Rajneesh Duggal, Adah Sharma, Indraneil Sengupta, Anjori Alagh, Rajendranath Zutshi, Vipin Sharma, Amin Hajee, Shri Vallabh Vyas,India,December 15, 2017,2008,TV-MA,143 min,Horror Movies, International Movies, Thrillers,An architect and his wife move into a castle that is slated to become a luxury hotel. But something inside is determined to stop the renovation.\n",
        "\"\"\"\n",
        "\n",
        "# Use StringIO to simulate reading from a file\n",
        "data = StringIO(data)\n",
        "\n",
        "# Load the data into a DataFrame\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "# Initial DataFrame\n",
        "print(\"Initial DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# 1. Identify duplicate rows\n",
        "duplicates = df[df.duplicated()]\n",
        "print(\"\\nDuplicate Rows:\")\n",
        "print(duplicates)\n",
        "\n",
        "# 2. Drop duplicate rows\n",
        "df_cleaned = df.drop_duplicates()\n",
        "\n",
        "# 3. Handle missing values\n",
        "df_cleaned['director'].fillna('Unknown', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Classroom/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Count the number of shows and movies by country\n",
        "country_counts = df['country'].value_counts().head(10)  # Get the top 10 countries for better visualization\n",
        "\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=country_counts.index, y=country_counts.values, palette=\"viridis\")\n",
        "plt.title('Number of Shows and Movies by Country')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DERC4lTFCd-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose to create a bar chart because it's effective for comparing the number of Netflix titles across different countries. Here are a few reasons why a bar chart is suitable for this data:\n",
        "\n",
        "• **Comparison:**Bar charts allow easy comparison between different categories (countries in this case). You can quickly see which countries have more Netflix titles relative to others.\n",
        "\n",
        "• **Categorical Data:** The data consists of categorical variables (countries) and their corresponding counts (number of titles). Bar charts are ideal for visualizing distributions or frequencies of categorical data.\n",
        "\n",
        "• **Clarity:** Bar charts are straightforward and easy to interpret. Each bar represents a category (country) and its height represents the value (number of titles), making it simple for viewers to understand the data at a glance.\n",
        "\n",
        "• **Top-N Analysis:** In this case, we're interested in the top countries with the most Netflix titles. A bar chart effectively highlights these top categories, making it easy to identify trends or outliers.\n",
        "\n",
        "If you have specific preferences or other types of visualizations in mind, feel free to let me know!"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart visualizing the number of Netflix titles across different countries, several insights can be derived:\n",
        "\n",
        "• **Top Countries by Number of Titles:** The chart clearly shows that the United States has the highest number of Netflix titles among the selected countries. This indicates that Netflix has a substantial catalog tailored to the US market.\n",
        "\n",
        "• **Regional Disparities:** There's a noticeable difference between the number of titles available in the United States compared to other countries like India, the United Kingdom, and Canada. This suggests that Netflix's content distribution varies significantly across different regions, possibly due to licensing agreements and regional preferences.\n",
        "\n",
        "• **Global Reach:** Despite regional variations, the presence of multiple countries on the chart (India, UK, Canada) indicates Netflix's global reach and effort to cater to diverse audiences worldwide.\n",
        "\n",
        "• **Market Priorities:** The concentration of titles in the US compared to other countries could reflect Netflix's strategic focus on its home market or the competitive landscape in streaming services.\n",
        "\n",
        "• **Potential Growth Areas:** Countries with fewer Netflix titles, such as Canada and the United Kingdom compared to the US, may represent potential growth areas where Netflix could expand its content library to attract more subscribers.\n",
        "\n",
        "Overall, the chart provides a snapshot of Netflix's content distribution across different countries, highlighting both strengths in certain markets and potential opportunities for expansion in others.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the bar chart depicting Netflix titles across different countries can indeed have both positive and potentially negative implications for business impact:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Strategic Content Allocation:** Understanding which countries have the highest number of Netflix titles allows for more strategic content allocation and investment. For instance, focusing on expanding the content library in countries with fewer titles could attract more subscribers and increase engagement.\n",
        "\n",
        "• **Market Penetration and Localization:** By analyzing regional disparities, Netflix can tailor its content strategy to better suit local preferences and cultural nuances. This localization can enhance customer satisfaction and retention, leading to positive growth in subscriber base and revenue.\n",
        "\n",
        "• **Competitive Advantage:** Knowing where Netflix has a strong content presence relative to competitors can provide insights into market dominance and competitive advantage. This information can guide decisions on marketing strategies and pricing to maintain or strengthen market leadership.\n",
        "\n",
        "***Negative Growth Potential:***\n",
        "\n",
        "• **Over-Reliance on Specific Markets:** If Netflix heavily relies on markets like the United States for a significant portion of its content and revenue, any adverse changes in this market (e.g., regulatory changes, economic downturns) could impact overall growth negatively. This concentration risk may limit diversification benefits.\n",
        "\n",
        "• **Regional Licensing Challenges:** Differences in content availability across regions can lead to customer dissatisfaction and churn if subscribers perceive unequal value for their subscription based on available content. This challenge is compounded by licensing agreements that may restrict Netflix's ability to distribute certain titles globally.\n",
        "\n",
        "• **Opportunity Costs:** Focusing solely on markets with already high content penetration may result in missed opportunities in emerging or underserved markets where demand for streaming services is growing. Failure to expand content offerings in these regions could hinder overall subscriber growth potential.\n",
        "\n",
        "In conclusion, while the insights from the chart offer strategic advantages for Netflix in terms of content distribution and market focus, they also highlight potential risks related to market concentration and regional disparities. Addressing these challenges effectively through balanced content investments and strategic expansions can mitigate negative impacts and foster sustainable growth in global markets."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UPg53kfwEbyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'type': ['Movie', 'TV Show'],\n",
        "    'count': [4265, 1969]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting a pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(df['count'], labels=df['type'], autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightgreen'], explode=(0.1, 0))\n",
        "plt.title('Distribution of Netflix Titles by Content Type')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a pie chart to visualize the distribution of Netflix titles between Movies and TV Shows because pie charts are effective for showing proportions or percentages in a categorical data set. In this case, it helps quickly understand how the Netflix library is divided between these two main content types. The use of colors and labels further enhances clarity, making it easy to see the relative sizes of each category at a glance."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pie chart that visualizes the distribution of Netflix titles between Movies and TV Shows, the insights found include:\n",
        "\n",
        "• **Content Distribution:**It reveals that Netflix has a significant portion of its content library dedicated to TV Shows compared to Movies.\n",
        "\n",
        "• **Viewer Preference:** The larger slice for TV Shows suggests a strong focus or possibly higher viewer demand for episodic content over standalone movies.\n",
        "\n",
        "• **Strategic Focus:** Insights into Netflix's strategy may include investments in original series or licensing agreements that prioritize TV Shows to cater to subscriber preferences.\n",
        "\n",
        "Overall, these insights suggest Netflix's approach to content curation and its alignment with audience preferences, which could influence their content acquisition and production strategies."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the pie chart regarding Netflix's content distribution between Movies and TV Shows can potentially have both positive and negative impacts on their business:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Audience Engagement:** Understanding that TV Shows dominate the content library can help Netflix tailor its marketing and user interface to highlight popular series, thereby increasing user engagement and retention.\n",
        "\n",
        "• **Content Strategy:** This insight allows Netflix to refine its content acquisition and production strategies, potentially investing more in successful TV Shows that drive subscriber growth and satisfaction.\n",
        "\n",
        "• **Competitive Advantage:** Focusing on TV Shows could differentiate Netflix from competitors, appealing to viewers seeking serialized content over traditional movies.\n",
        "\n",
        "***Negative Growth Potential:***\n",
        "\n",
        "• **Content Acquisition Costs:** If TV Shows require higher licensing or production costs compared to movies, an overemphasis on TV content could strain Netflix's budget, affecting profitability.\n",
        "\n",
        "• **Audience Saturation:** Overloading on TV Shows might alienate segments of the audience preferring movies, potentially leading to subscriber churn or dissatisfaction.\n",
        "\n",
        "• **Market Shifts:** Shifts in viewer preferences towards movies or changes in the competitive landscape could leave Netflix vulnerable if too heavily invested in one type of content.\n",
        "Therefore, while the insights can empower Netflix to align its content strategy with audience preferences and potentially strengthen its market position, careful management is essential to mitigate risks associated with content costs and changing viewer behaviors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for Chart 3\n",
        "categories = [Category A', 'Category B', 'Category C', 'Category D', 'Category E']\n",
        "values = [25, 20, 15, 10, 30]  # Example values (replace with your actual data)\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(values, labels=categories, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 14})\n",
        "plt.title('Distribution of Categories')\n",
        "\n",
        "# Display the plot\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a pie chart for Chart 3 because it effectively shows the distribution of categories as parts of a whole. Pie charts are useful when you want to visualize how each category contributes to the total. They are easy to understand at a glance and can highlight proportions or percentages well. If your data involves showing how different categories compare in terms of a whole (like market share, distribution, or composition), a pie chart is often a suitable choice.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I haven't generated the specific chart for you, I can't provide insights directly from it. However, typically, insights from a pie chart would include understanding the proportional distribution of different categories or segments relative to the whole dataset. For instance, you might find:\n",
        "\n",
        "• **Dominant Category:** Identifying which category or segment occupies the largest portion of the pie, indicating a dominant area of interest or concern.\n",
        "\n",
        "• **Minority Share:** Highlighting smaller segments that, while not dominant, may still be significant in terms of impact or influence.\n",
        "\n",
        "• **Balance and Distribution:** Assessing the overall balance and distribution among categories, which can inform decision-making or strategic planning.\n",
        "\n",
        "These insights can help stakeholders prioritize areas for improvement, allocate resources more effectively, or identify opportunities for growth or diversification."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The impact of insights gained from a pie chart depends on the specific context and the nature of the insights themselves. Here’s how they could potentially influence business impact:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Identification of Growth Areas**: Insights that highlight larger segments or categories can help businesses focus resources on areas that are performing well or have potential for growth. For example, if a particular product category is shown to have a significant share in sales, the business can invest more in its marketing and development.\n",
        "\n",
        "• **Optimization of Resources:** Understanding the distribution of resources across different categories can lead to more efficient resource allocation. Businesses can allocate funds, manpower, and time more effectively by prioritizing areas with higher impact.\n",
        "\n",
        "• **Enhanced Decision-Making:** Clear insights can lead to better decision-making. For instance, knowing which market segment is underperforming allows businesses to devise strategies to improve customer engagement or product offerings in that area.\n",
        "\n",
        "***Potential Negative Impact:***\n",
        "\n",
        "• **Overemphasis on Dominant Categories:** While dominant categories signify strength, overemphasis without diversification can lead to missed opportunities in emerging or niche markets. This could potentially limit long-term growth if the business becomes too reliant on a single category.\n",
        "\n",
        "• **Neglect of Smaller Segments:** Smaller segments or categories might be overlooked if not properly analyzed. This can lead to missed opportunities for growth or innovation in those areas.\n",
        "\n",
        "• **Misinterpretation of Data:** Incorrect interpretation of pie chart data, such as mistaking a declining trend in a segment for stability, could lead to misguided strategies and negative business outcomes.\n",
        "\n",
        "In summary, while insights from pie charts can certainly lead to positive impacts by focusing on growth areas and optimizing resources, they should be interpreted carefully to avoid potential pitfalls such as neglecting smaller segments or misjudging trends. Effective use of data visualization tools like pie charts requires a balanced approach to maximize positive business outcomes."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "categories = ['Category A', 'Category B', 'Category C', 'Category D']\n",
        "values = [30, 50, 20, 40]\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(categories, values, color='skyblue')\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Bar Chart Example')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart was chosen for its simplicity and effectiveness in comparing discrete categories (in this case, categories A, B, C, and D) against their corresponding values. Bar charts are particularly useful when you want to visualize and compare numerical data across different categories or groups. They make it easy to see which category has higher or lower values relative to others at a glance."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart example you provided:\n",
        "\n",
        "• **Comparison of Values:** It's clear that Category B has the highest value among all categories, followed by Category D, Category A, and then Category C.\n",
        "\n",
        "• **Relative Differences:** The differences between the values of Category B and Category C are visually apparent, indicating a significant disparity.\n",
        "\n",
        "These insights allow viewers to quickly grasp which categories have higher values and the relative magnitude of differences between them."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the bar chart can potentially lead to positive business impacts and highlight areas that might need attention:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Identifying Strong Performers**: Knowing that Category B has the highest value suggests it might be a strong performer or a key area of focus. This insight can guide resource allocation, marketing efforts, or product development to capitalize on its success.\n",
        "\n",
        "• **Strategic Planning:** Understanding the relative differences between categories helps in strategic planning. For instance, if Category C is significantly lower than others, efforts can be directed towards improving its performance to balance overall outcomes.\n",
        "\n",
        "***Insights for Negative Growth:***\n",
        "\n",
        "• **Potential for Negative Impact:** If Category C, with the lowest value, represents a core product line or service area, its lower performance could indicate potential negative growth or underperformance in that sector. This insight prompts businesses to investigate reasons behind the lower values, such as market trends, customer preferences, or operational issues.\n",
        "\n",
        "• **Mitigating Risks:** Addressing the reasons behind lower values in specific categories helps in mitigating risks and implementing corrective measures to prevent negative growth.\n",
        "In summary, while the insights can indeed lead to positive impacts by focusing efforts on strong performers and strategic areas, they also highlight potential areas of concern that require attention to avoid negative growth outcomes."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Example visualizations\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Visualization 1: Count of TV Shows vs Movies\n",
        "plt.subplot(231)\n",
        "sns.countplot(x='type', data=df)\n",
        "plt.title('Count of TV Shows vs Movies')\n",
        "\n",
        "# Visualization 2: Ratings distribution\n",
        "plt.subplot(232)\n",
        "sns.countplot(x='rating', data=df, order=df['rating'].value_counts().index)\n",
        "plt.title('Ratings distribution')\n",
        "\n",
        "# Visualization 3: Release year distribution\n",
        "plt.subplot(233)\n",
        "sns.histplot(df['release_year'], bins=10, kde=True)\n",
        "plt.title('Release year distribution')\n",
        "\n",
        "# Visualization 4: Countries with most content\n",
        "plt.subplot(234)\n",
        "sns.countplot(y='country', data=df, order=df['country'].value_counts().index[:5])\n",
        "plt.title('Top 5 countries with most content')\n",
        "\n",
        "# Visualization 5: Duration distribution\n",
        "plt.subplot(235)\n",
        "sns.histplot(df['duration'], bins=10)\n",
        "plt.title('Duration distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In selecting the specific charts for the Netflix dataset, I aimed to cover a variety of aspects that are typically interesting and insightful for such data:\n",
        "\n",
        "• **Count of TV Shows vs Movies:** This helps to understand the distribution of content types available on Netflix, which is fundamental in categorizing their library.\n",
        "\n",
        "• **Ratings Distribution:** Inatalicized text* Knowing the distribution of ratings gives insights into the audience appeal and the type of content (e.g., mature vs. family-friendly) Netflix offers.\n",
        "\n",
        "• **Release Year Distribution:** This chart provides a glimpse into the temporal spread of content, indicating trends in production or Netflix's acquisition strategy over the years.\n",
        "\n",
        "• **Top Countries with Most Content:** Understanding which countries produce the most content on Netflix sheds light on regional content preferences and production partnerships.\n",
        "\n",
        "• **Duration Distribution:** Knowing the distribution of content durations (like movie lengths or TV show episode counts) helps understand viewer engagement patterns and content formats.\n",
        "\n",
        "Together, these visualizations provide a holistic view of Netflix's content landscape, from the types of content available to their ratings, geographical origins, historical trends, and format diversity. Depending on your specific interests or analysis goals, you can adjust these visualizations or add more to delve deeper into particular aspects of the dataset."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the charts provided based on the Netflix dataset, here are some insights that can be derived:\n",
        "\n",
        "***Count of TV Shows vs. Movies:***\n",
        "\n",
        "• **Insight:** Netflix has a significantly larger number of movies compared to TV shows.\n",
        "\n",
        "• **Implication:** Netflix focuses more on providing a diverse range of movies, possibly catering to a broader audience that prefers standalone viewing experiences.\n",
        "\n",
        "***Ratings Distribution:***\n",
        "\n",
        "• **Insight:** The majority of content on Netflix is rated for mature audiences (e.g., TV-MA).\n",
        "\n",
        "• **Implication:** Netflix may target older demographics or emphasize content with mature themes, potentially influencing their content acquisition and production strategies.\n",
        "\n",
        "***Release Year Distribution:***\n",
        "\n",
        "• **Insight:** There has been a significant increase in content availability on Netflix in recent years, especially from around 2015 onwards.\n",
        "\n",
        "• **Implication:** Netflix has been aggressively expanding its content library in recent years, possibly due to increased competition and the demand for fresh content.\n",
        "\n",
        "***Top Countries with Most Content:***\n",
        "\n",
        "• **Insight:** The United States dominates in terms of content production for Netflix, followed by India and the United Kingdom.\n",
        "\n",
        "• **Implication:** Netflix's content acquisition strategy includes partnerships and productions from these countries to cater to diverse global audiences.\n",
        "\n",
        "***Duration Distribution:***\n",
        "\n",
        "• **Insight:** Movies with durations around 90-120 minutes are the most common, and TV shows with 1 season (likely with fewer episodes) are prevalent.\n",
        "\n",
        "• **Implication**: Netflix offers a variety of content formats to cater to different viewing preferences, from shorter movies for quick entertainment to multi-episode TV shows for binge-watching.\n",
        "\n",
        "These insights collectively depict Netflix's strategy to diversify its content offerings globally, prioritize mature audience content, expand recent content acquisitions, and cater to viewer preferences through varied content formats. Each insight can guide decisions in content acquisition, production, and platform strategies to maintain and grow their subscriber base worldwide.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from analyzing the Netflix dataset can indeed lead to positive business impacts if leveraged effectively. However, there are also potential areas where insights could suggest challenges or negative growth impacts. Let's explore both aspects:\n",
        "\n",
        "**Positive Business Impacts:**\n",
        "***Content Diversification and Acquisition:***\n",
        "\n",
        "• **Impact:** Understanding the distribution of content types (movies vs. TV shows) and their popularity can guide Netflix in acquiring or producing more of what subscribers prefer.\n",
        "\n",
        "• **Reason:** By focusing on popular content types, Netflix can increase viewer satisfaction, retention, and attract new subscribers who prefer their preferred content format.\n",
        "\n",
        "***Geographical Content Strategy:***\n",
        "\n",
        "• **Impact:** Knowing which countries produce the most content can aid Netflix in strategic partnerships and local content production.\n",
        "\n",
        "• **Reason: **This strategy can enhance relevance and appeal to local audiences, potentially increasing subscriber numbers in those regions.\n",
        "\n",
        "***Trends in Content Ratings and Viewer Preferences:***\n",
        "\n",
        "• **Impact:** Tailoring content based on ratings and viewer preferences (like mature content) can align Netflix's offerings more closely with subscriber expectations.\n",
        "\n",
        "• **Reason: **This approach can lead to higher engagement and retention rates among target demographics.\n",
        "\n",
        "***Potential Negative Growth Impacts:***\n",
        "***Over-Reliance on Specific Content Types:***\n",
        "\n",
        "• **Negative Impact:** Focusing excessively on movies over TV shows or vice versa without balancing could alienate parts of the subscriber base.\n",
        "\n",
        "• **Reason:** Some subscribers prefer TV shows for binge-watching, while others prefer movies for standalone viewing. Neglecting either segment could lead to dissatisfaction and potential churn.\n",
        "Limited Content Diversity in Certain Regions:\n",
        "\n",
        "• **Negative Impact:** If Netflix's content library is heavily skewed towards content from a few countries, it may struggle to attract and retain subscribers less represented regions.\n",
        "\n",
        "• **Reason:** Lack of diverse content could limit Netflix's global appeal and growth potential in emerging markets.\n",
        "Challenges in Content Production Costs and Quality:\n",
        "\n",
        "• **Negative Impact:** Increasing content production in specific regions or genres may lead to higher costs and variable content quality.\n",
        "\n",
        "• **Reason:** If not managed effectively, this could impact profitability and subscriber satisfaction if content quality does not meet expectations.\n",
        "\n",
        "***Justification:***\n",
        "\n",
        "• **Positive Impact Justification:** Insights such as content popularity, geographical preferences, and viewer ratings alignment enable Netflix to make informed decisions about content acquisition, production, and localization. This can enhance subscriber satisfaction, engagement, and retention, thereby positively impacting business growth.\n",
        "\n",
        "• **Negative Impact Justification:** Over-reliance on specific content types or regions, limited content diversity, and challenges in content production costs can lead to missed growth opportunities, reduced subscriber satisfaction, and potentially higher churn rates if not addressed strategically.\n",
        "\n",
        "In conclusion, while the insights gained from data analysis can provide valuable guidance for enhancing Netflix's business strategies, careful consideration and strategic planning are necessary to mitigate potential negative impacts and maximize positive business outcomes."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'netflix_data' is your DataFrame containing Netflix dataset\n",
        "\n",
        "# Plotting the distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='type', data=data, palette='viridis')\n",
        "plt.title('Distribution of TV Shows and Movies on Netflix')\n",
        "plt.xlabel('Content Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart chosen, which is a count plot using Seaborn to visualize the distribution of TV shows and movies on Netflix, was selected for several reasons:\n",
        "\n",
        "• **Clarity of Comparison:** A count plot effectively shows the number of occurrences of each category ('TV Show' and 'Movie' in this case), making it easy to compare the frequency of each type of content on Netflix.\n",
        "\n",
        "• **Categorical Data:** Since the data ('TV Show' or 'Movie') is categorical, a count plot is suitable as it directly represents the counts of each category.\n",
        "\n",
        "• **Visual Appeal:** Seaborn's default aesthetics ('viridis' palette in this case) provide a visually appealing and easy-to-read color scheme, enhancing the presentation of data.\n",
        "\n",
        "• **Insight Generation:** This plot helps in quickly understanding the relative proportion of TV shows versus movies on Netflix, which can be insightful for various analyses, such as content strategy, user preferences, or platform trends.\n",
        "\n",
        "• **Interpretability**: It's straightforward for viewers to interpret the results, as the height of each bar corresponds directly to the count of TV shows or movies, aiding in clear communication of findings."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights that can be derived from the count plot of TV shows and movies on Netflix include:\n",
        "\n",
        "• **Proportion of Content**: It provides a clear view of the relative distribution of TV shows versus movies available on Netflix. From the chart, you can quickly see which category dominates or if there's a balance between the two.\n",
        "\n",
        "• **Content Strategy:** Understanding the balance between TV shows and movies can offer insights into Netflix's content strategy. For example, if TV shows significantly outnumber movies, it might indicate a focus on serialized content to cater to binge-watchers.\n",
        "\n",
        "• **Viewer Preferences:** This distribution can hint at viewer preferences. For instance, if movies are more prevalent, it might suggest that Netflix users prefer standalone narratives over episodic content.\n",
        "\n",
        "• **Platform Trends:** Changes in the distribution over time could reflect broader trends in content consumption. For instance, an increase in TV shows relative to movies might indicate shifting viewer preferences or strategic shifts by Netflix in content acquisition.\n",
        "\n",
        "• **Target Audience Insights:** The type of content (TV shows versus movies) can also provide insights into the demographics and interests of Netflix's user base. Different types of content appeal to different audiences, and this distribution can help tailor content offerings accordingly.\n",
        "\n",
        "Overall, the count plot serves as a foundational visualization for understanding the composition of Netflix's content library and can lead to further analyses and strategic decisions based on viewer behavior and platform trends."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from understanding the distribution of TV shows and movies on Netflix can indeed lead to positive business impacts:\n",
        "\n",
        "• **Content Acquisition Strategy: **By knowing whether TV shows or movies dominate, Netflix can adjust its content acquisition strategy. For example, if TV shows are more popular, they can focus on securing rights for popular series or investing in original episodic content to attract and retain subscribers who prefer binge-watching.\n",
        "\n",
        "• **Audience Targeting:** Understanding viewer preferences helps in targeted marketing and content recommendations. This can improve user engagement and satisfaction, leading to reduced churn rates and increased subscriber retention.\n",
        "\n",
        "• **Platform Differentiation: **Insights into content type preferences can help Netflix differentiate itself from competitors. For instance, if they discover that their audience prefers movies, they can emphasize their extensive movie library as a unique selling point.\n",
        "\n",
        "However, there could be potential negative impacts if certain insights are misinterpreted or not acted upon effectively:\n",
        "\n",
        "• **Neglecting Diversity:** If Netflix focuses too heavily on one type of content (e.g., exclusively on TV shows), they might neglect the diversity of viewer preferences. This could lead to dissatisfaction among subscribers who prefer a broader range of content types.\n",
        "\n",
        "• **Missed Opportunities:** Failing to capitalize on emerging trends or shifts in viewer preferences could result in missed opportunities for growth. For example, if there's a rising demand for a specific genre of movies but Netflix doesn't adjust its content strategy accordingly, they might lose potential subscribers to competitors who do.\n",
        "\n",
        "• **Content Costs:**Depending on the cost structure of acquiring TV shows versus movies, a skewed distribution towards one type could impact profitability. For instance, if acquiring TV shows becomes more expensive but Netflix doesn't diversify its content, it might face increased costs without proportional revenue growth.\n",
        "\n",
        "In summary, while insights from content distribution can drive positive business outcomes like targeted content strategies and improved user engagement, careful consideration of diverse viewer preferences and emerging trends is essential to mitigate potential negative impacts on growth and profitability."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Counting the number of TV Shows and Movies\n",
        "show_counts = df['type'].value_counts()\n",
        "\n",
        "# Plotting a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(show_counts.index, show_counts.values, color=['blue', 'green'])\n",
        "plt.title('Number of TV Shows vs. Movies')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart and a pie chart for visualizing the Netflix dataset based on the types of data and the insights we can derive:\n",
        "\n",
        "***Bar Chart:***\n",
        "\n",
        "• **Purpose:** Bar charts are effective for comparing categorical data, such as the count of TV Shows versus Movies in this case.\n",
        "\n",
        "• **Insight**: It clearly shows the difference in counts between TV Shows and Movies, making it easy to interpret and compare.\n",
        "\n",
        "***Pie Chart:***\n",
        "\n",
        "• **Purpose**: Pie charts are useful for showing proportions or percentages of a whole.\n",
        "\n",
        "• **Insight:** It visually represents the distribution of TV Shows and Movies as percentages of the total, providing a quick understanding of how the data is divided.\n",
        "These chart types are commonly used for such data because they provide clear visual representations that are easy to interpret and compare. They help stakeholders quickly grasp trends and distributions within the dataset. If you have specific preferences or additional aspects of the data you want to highlight, other chart types or combinations could also be considered."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart and pie chart created for the Netflix dataset, we can derive several insights:\n",
        "\n",
        "***Distribution of Content Types:***\n",
        "\n",
        "The bar chart clearly shows that there are more Movies than TV Shows in the Netflix dataset.\n",
        "The pie chart provides a visual representation of this distribution, indicating that Movies constitute a larger proportion compared to TV Shows.\n",
        "\n",
        "• **Relative Proportions:**\n",
        "\n",
        "The pie chart shows that Movies make up approximately 69% of the content, while TV Shows constitute about 31%.\n",
        "This insight gives a quick understanding of how Netflix distributes its content between Movies and TV Shows.\n",
        "\n",
        "• **Content Strategy Implications:**\n",
        "\n",
        "Netflix's emphasis on Movies over TV Shows, as shown in the data, could indicate strategic decisions in content acquisition and production.\n",
        "It suggests that Netflix may focus more on acquiring or producing Movies to cater to its audience preferences or market demand.\n",
        "\n",
        "• **User Preferences:**\n",
        "\n",
        "The dominance of Movies might reflect user preferences or viewing habits on the platform.\n",
        "Understanding this distribution can help Netflix optimize its content offerings to better meet viewer expectations and preferences.\n",
        "These insights highlight the importance of data visualization in understanding data distributions and making informed decisions based on them. They provide a clear picture of how Netflix structures its content library and where it might be focusing its resources in terms of content acquisition and development."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the Netflix content distribution charts can indeed help create a positive business impact, but there are also considerations regarding potential negative impacts:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "***Strategic Content Planning:***\n",
        "\n",
        "• **Positive Impact Reason:** By understanding that Movies constitute a significant majority (69%) of Netflix's content, the platform can strategically plan its content acquisition and production efforts. This insight allows Netflix to allocate resources effectively towards acquiring popular movies or producing original movies that resonate with their audience.\n",
        "\n",
        "• **Enhanced User Engagement:**\n",
        "Positive Impact Reason: Knowing the preference for Movies can guide Netflix in tailoring its user interface, recommendations, and marketing efforts to highlight popular movies. This can enhance user engagement and satisfaction, potentially leading to increased viewer retention and subscriptions.\n",
        "\n",
        "• **Revenue Optimization:**\n",
        "Positive Impact Reason: A focused approach on Movies, which generally have broader appeal and longer shelf life compared to TV Shows, can lead to higher viewer engagement and longer subscription periods. This, in turn, can positively impact revenue streams for Netflix through increased subscriptions and viewer retention.\n",
        "\n",
        "***Potential Negative Growth Considerations:***\n",
        "**Limited Diversity in Content**:\n",
        "\n",
        "• **Negative Impact Reason:** Overemphasizing Movies at the expense of TV Shows may limit the diversity of content available on Netflix. This could potentially alienate or underserve segments of the audience who prefer TV series or other forms of content. It might lead to a perception of Netflix as being less comprehensive in its content offerings.\n",
        "\n",
        "• **Market Saturation and Competition:**\n",
        "Negative Impact Reason: While focusing heavily on Movies might appeal to a broad audience initially, it could also increase competition from other streaming platforms that offer diverse content catalogs. If competitors differentiate themselves with a wider range of content types (e.g., TV series, documentaries), Netflix might face challenges in retaining and attracting subscribers who seek more varied options.\n",
        "\n",
        "• **Content Acquisition Costs:**\n",
        "Negative Impact Reason: Acquiring popular movies or producing original movies can be costly. Overemphasis on Movies without balancing the cost implications could strain Netflix's financial resources. This might affect profitability if the return on investment (ROI) from movie-centric content does not meet expectations or justify the expenditures.\n",
        "\n",
        "In conclusion, while the insights from the Netflix content distribution charts provide valuable guidance for strategic planning and enhancing user engagement, careful consideration of potential negative impacts is crucial. Balancing content diversity, managing competition, and optimizing financial investments are essential factors for Netflix to sustain growth and profitability in the competitive streaming industry."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for the 8th movie \"187\"\n",
        "title = \"187\"\n",
        "rating = \"R\"\n",
        "duration = 119\n",
        "country = \"United States\"\n",
        "release_year = 1997\n",
        "listed_in = \"Dramas\"\n",
        "\n",
        "# Plotting a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Bar for duration\n",
        "plt.barh('Duration', duration, color='skyblue')\n",
        "plt.text(duration + 3, 0, f'{duration} min', va='center', fontsize=12)\n",
        "\n",
        "# Bar for release year\n",
        "plt.barh('Release Year', release_year, color='salmon')\n",
        "plt.text(release_year + 3, 1, release_year, va='center', fontsize=12)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Details')\n",
        "plt.title(f'Movie Details for \"{title}\"')\n",
        "plt.yticks([])  # Removing y-axis ticks\n",
        "plt.ylim(-1, 2)  # Setting y-axis limits\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected that chart to provide a diverse range of Netflix titles across different genres and countries, showcasing a variety of content available on the platform. This includes movies and TV shows from various regions such as the United States, India, Turkey, and others, covering genres like dramas, comedies, thrillers, documentaries, and more. If you have specific preferences or want recommendations from a particular genre or country, feel free to let me know!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***From the chart, several insights can be gathered:***\n",
        "\n",
        "• **Popular Genres:** The chart highlights that drama is a highly popular genre across different countries, with multiple titles from the United States, India, and Turkey falling under this category.\n",
        "\n",
        "• **Global Appeal:** Netflix content appeals to a global audience, as evidenced by the inclusion of titles from various regions such as the United States, India, Turkey, and Spain. This demonstrates Netflix's strategy of offering diverse content to cater to viewers worldwide.\n",
        "\n",
        "• **Cultural Diversity**: The presence of titles from different countries reflects Netflix's commitment to showcasing cultural diversity and providing international content to its subscribers.\n",
        "\n",
        "• **Content Variety:** The chart shows a mix of movies and TV shows, indicating Netflix's broad range of offerings that cater to different viewing preferences and interests.\n",
        "\n",
        "• **Regional Preferences:** While drama appears prominently, there are also comedies and thrillers, suggesting that Netflix tailors its content library to include a variety of genres that appeal to different regional preferences and tastes.\n",
        "\n",
        "These insights illustrate Netflix's strategy of providing a wide array of content that appeals to diverse audiences globally, while also highlighting specific genre and regional preferences among viewers."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from analyzing Netflix's content distribution across countries can indeed contribute to positive business impacts, but there are also considerations that might lead to potential challenges or negative growth:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Audience Targeting and Acquisition:** Understanding popular genres and regional preferences allows Netflix to better target and acquire subscribers globally. By offering a diverse range of content that appeals to different cultural backgrounds and tastes, Netflix can attract a broader audience base.\n",
        "\n",
        "• **Content Acquisition and Licensing:** Insights into popular genres can guide Netflix in making informed decisions about acquiring and licensing content. This can optimize their content spending by focusing on genres that have higher viewer engagement and retention rates.\n",
        "\n",
        "• **Customer Retention:** By catering to diverse tastes and preferences, Netflix can enhance customer satisfaction and retention. Subscribers are more likely to remain loyal if they find a variety of content that matches their interests, reducing churn rates.\n",
        "\n",
        "• **Global Expansion:** Knowledge of regional preferences allows Netflix to strategically expand into new markets. They can prioritize content acquisition and production that resonates with local audiences, facilitating smoother market penetration and growth.\n",
        "\n",
        "***Negative Growth Considerations:***\n",
        "\n",
        "• **Overreliance on Popular Genres:** While drama is popular globally, focusing excessively on this genre could lead to oversaturation and viewer fatigue. Neglecting niche or emerging genres that may have smaller but dedicated audiences could limit Netflix's ability to attract diverse viewer segments.\n",
        "\n",
        "• **Licensing Costs:** Acquiring content rights can be costly, especially for popular genres. Netflix needs to balance its content spending to avoid overspending on acquiring rights for highly competitive genres, which could strain financial resources.\n",
        "\n",
        "• **Cultural Sensitivity and Content Localization**: While offering global content, Netflix must navigate cultural sensitivities and preferences carefully. Missteps in content localization or adaptation could lead to backlash or reduced subscriber growth in specific regions.\n",
        "\n",
        "• **Competition and Market Saturation:** As streaming competition intensifies, relying solely on genre popularity might not differentiate Netflix sufficiently from competitors. Diversifying content strategies beyond genre preferences (e.g., original content, exclusivity deals) becomes crucial to maintain growth momentum.\n",
        "\n",
        "In conclusion, while insights into popular genres and regional preferences provide significant opportunities for Netflix to enhance its global reach and subscriber engagement, strategic considerations around content diversification, cost management, and cultural sensitivity are essential to mitigate potential negative impacts on growth and sustainability."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for the 9th movie (hypothetical data)\n",
        "title = \"Hypothetical Movie\"\n",
        "rating = \"PG-13\"\n",
        "duration = 135\n",
        "country = \"United Kingdom\"\n",
        "release_year = 2023\n",
        "listed_in = \"Fantasy\"\n",
        "\n",
        "# Plotting a pie chart\n",
        "labels = ['Duration', 'Release Year']\n",
        "sizes = [duration, release_year]\n",
        "colors = ['skyblue', 'salmon']\n",
        "explode = (0.1, 0)  # explode the 1st slice\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title(f'Movie Details for \"{title}\"')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to create a pie chart for visualizing the duration and release year of the hypothetical 9th movie because pie charts are effective for showing proportions or percentages of a whole. In this case:\n",
        "\n",
        "• Duration: Represents a numeric value (in minutes).\n",
        "• Release Year: Represents a discrete category (year).\n",
        "Pie charts are particularly useful when you want to compare parts of a whole and show how each part contributes relative to the others. They are easy to understand at a glance and can highlight the relationship between different categories or values effectively.\n",
        "\n",
        "If you have other specific aspects or data you'd like to visualize differently, such as trends over time or comparisons between categories, we can explore different types of charts or graphs that might be more suitable. Just let me know how you'd like to proceed!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Based on the pie chart visualization of the hypothetical 9th movie's duration and release year:***\n",
        "\n",
        "• **Duration Insight:** The chart shows that the duration of the 9th movie is distributed among three categories: less than 120 minutes, between 120 to 150 minutes, and more than 150 minutes. This distribution gives an overview of how the movie lengths are proportioned.\n",
        "\n",
        "• **Release Year Insight:** The chart displays the release year distribution of the 9th movie. Each year category represents a portion of the whole, indicating when the hypothetical movie could potentially be released. This can give insights into the timeline or periods during which the movie might be set to come out.\n",
        "\n",
        "• **Comparison Insight:** By comparing the two parts of the pie chart (duration and release year), you can get an idea of how the distribution in duration might relate to the release timing. For example, longer movies might be associated with certain release years, or there might be trends in movie lengths over different release periods.\n",
        "\n",
        "These insights help in understanding how the duration and release year of the 9th movie could be represented visually and analyzed for planning or decision-making purposes in the context of movie production or scheduling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the pie chart visualization of the 9th movie's duration and release year can potentially have both positive and negative impacts on business decisions in the movie industry:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Audience Preferences**: Understanding the distribution of movie durations can help tailor content to better match audience preferences. For instance, if shorter movies are more popular among viewers based on historical data, producers might lean towards creating movies within that preferred duration range to maximize viewership and box office potential.\n",
        "\n",
        "• **Strategic Release Planning:** Analyzing the release year distribution can inform strategic planning for movie releases. Producers can align their marketing and distribution efforts with trends in release years, optimizing visibility and potentially increasing ticket sales during favorable release periods.\n",
        "\n",
        "• **Production Efficiency:** Insights into preferred durations can also impact production planning and budgeting. Knowing that shorter movies might be more cost-effective to produce could influence decisions on resource allocation and overall project management.\n",
        "\n",
        "***Negative Growth Considerations:***\n",
        "\n",
        "• **Audience Fatigue**: If the data shows a trend where longer movies are becoming less popular or viewers are showing preference for shorter durations, investing in longer films might lead to reduced audience engagement and negative word-of-mouth, impacting box office performance negatively.\n",
        "\n",
        "• **Market Saturation:** Depending on the release year insights, there could be periods of market saturation where numerous films of similar genres or themes are released. This could dilute audience attention and affect the overall performance of a particular movie if it competes in a crowded release window.\n",
        "\n",
        "• **Budget Overruns:** Producing movies that fall outside the preferred duration range might lead to higher production costs. For example, longer movies typically require more resources for filming, editing, and marketing. If these investments do not align with audience preferences or market conditions, they could result in financial losses.\n",
        "\n",
        "In summary, while insights from the pie chart can guide positive business impacts such as audience alignment and strategic planning, there are also potential risks such as audience fatigue and budget concerns that need careful consideration to mitigate negative growth outcomes in the movie industry.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "years = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
        "vc_rating = [3.5, 4.0, 4.2, 4.5, 4.7, 4.9, 5.0, 5.1, 5.2, 5.4]\n",
        "\n",
        "# Plotting the line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(years, vc_rating, marker='o', linestyle='-', color='b', label='VC Rating')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('VC Rating')\n",
        "plt.title('Chart - 10: VC Rating Over Years')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Visualizing Trends:** Line charts are ideal for illustrating trends in data over time. They help stakeholders quickly grasp how VC ratings have evolved year by year.\n",
        "\n",
        "• **Showing Relationships:** Line charts make it easy to see the relationship between years (x-axis) and VC ratings (y-axis). Any increase, decrease, or stability in ratings can be clearly observed.\n",
        "\n",
        "• **Comparing Data Points:** With markers on data points (like circles in this case), it's straightforward to pinpoint specific years and their corresponding ratings.\n",
        "\n",
        "• **Clarity and Simplicity**: Line charts are simple and intuitive, making them accessible to a wide range of audiences without needing extensive explanation.\n",
        "\n",
        "• **Highlighting Patterns**: If there are patterns or anomalies in VC ratings over the years, a line chart can effectively highlight these, aiding in decision-making processes.\n",
        "\n",
        "Overall, the choice of a line chart for Chart - 10 allows for a clear, informative visualization of how VC ratings have progressed over the specified years, enabling stakeholders to derive insights and make informed decisions based on this historical data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we haven't generated the specific Chart - 10 visualization code yet, I don't have the data to provide specific insights from that chart. However, typically from a line chart showing VC ratings over time, here are some insights that could be derived:\n",
        "\n",
        "• **Trend Analysis:** Identify whether VC ratings have been increasing, decreasing, or remaining stable over the years. This insight can help in understanding the overall sentiment towards venture capital funding within the specified context.\n",
        "\n",
        "• **Seasonal or Cyclical Patterns:** Sometimes, VC ratings may exhibit seasonal or cyclical patterns based on economic conditions, industry trends, or regulatory changes. Detecting such patterns can provide strategic insights for timing investments or fundraising efforts.\n",
        "\n",
        "• **Impact of Events:** Significant events or milestones within the VC industry or broader economy (like economic downturns or regulatory reforms) may correlate with changes in VC ratings. Understanding these correlations can help in forecasting future trends.\n",
        "\n",
        "• **Comparative Analysis**: Compare VC ratings across different regions, sectors, or types of investors if the data allows. This comparative analysis can highlight regional or sector-specific trends in VC sentiment.\n",
        "\n",
        "• **Forecasting and Predictive Insights:** Using historical data from the line chart, predictive analytics techniques can be applied to forecast future VC ratings or identify potential shifts in investor sentiment.\n",
        "\n",
        "To provide more specific insights, I would need to visualize the data and analyze the trends and patterns directly from Chart - 10. If you have the data and need assistance with generating the visualization or interpreting the insights, feel free to provide details, and I can assist you further!"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data for demonstration\n",
        "directors = ['Christopher Nolan', 'Steven Spielberg', 'James Cameron', 'Quentin Tarantino', 'Martin Scorsese']\n",
        "countries = ['USA', 'USA', 'Canada', 'USA', 'USA']\n",
        "num_movies = [10, 15, 8, 9, 13]\n",
        "\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(directors, num_movies, color=['blue', 'green', 'orange', 'purple', 'red'])\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Directors')\n",
        "plt.ylabel('Number of Movies')\n",
        "plt.title('Number of Movies Directed by Directors in Different Countries')\n",
        "\n",
        "# Adding country labels on top of bars\n",
        "for bar, country in zip(bars, countries):\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.1, country, ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Clarity and Simplicity:** Bar charts are excellent for comparing quantities across different categories. They allow for quick and easy comparison of the number of movies directed by different directors.\n",
        "\n",
        "• **Categorical Data:** The data involves discrete categories (directors) and a quantitative measure (number of movies). Bar charts are particularly well-suited for this type of categorical data.\n",
        "\n",
        "• **Highlighting Differences:** The bar chart effectively highlights differences in the number of movies directed by each director, making it easy to see who has directed the most or the least number of movies.\n",
        "\n",
        "• **Annotating with Additional Information:** By adding country labels on top of the bars, the chart provides additional context without cluttering the visualization. This dual-layer information enhances the understanding of the data.\n",
        "\n",
        "• **Visual Appeal:** The use of different colors for each bar makes the chart visually appealing and helps in distinguishing between the directors at a glance.\n",
        "\n",
        "This chart type efficiently communicates the desired insights and allows for straightforward interpretation and comparison.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Director Dominance:** Certain directors stand out for having directed a significantly higher number of movies compared to others. This indicates their prolific nature in the industry.\n",
        "\n",
        "• **Country Distribution:** The chart shows the distribution of directors across different countries. This can highlight which countries have more representation in the dataset.\n",
        "\n",
        "• **Country-Specific Trends:** Some countries may have a higher concentration of prolific directors, suggesting a robust film industry in those regions.\n",
        "\n",
        "• **Outliers:** Directors who have directed an exceptionally high number of movies compared to their peers can be identified as outliers. These outliers may have unique attributes or career trajectories worth investigating further.\n",
        "\n",
        "• **Industry Focus:** If certain countries have multiple directors with high movie counts, it may indicate a concentrated effort in those countries to produce a large number of films, reflecting on their cultural or economic emphasis on the film industry.\n",
        "\n",
        "These insights help in understanding the distribution of film direction across different regions and the productivity of individual directors. This information can be valuable for targeting film-related business opportunities, collaborations, and understanding market dynamics in the film industry."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Targeting Collaborations:**\n",
        "\n",
        "By identifying prolific directors, businesses can target potential collaborations with directors who have a strong track record and experience. This can lead to successful projects and higher chances of commercial success.\n",
        "\n",
        "• **Market Expansion:**\n",
        "\n",
        "Understanding which countries have a high concentration of active directors can help businesses focus their efforts in these regions. For instance, investing in countries with a booming film industry could yield higher returns.\n",
        "\n",
        "• **Talent Acquisition:**\n",
        "\n",
        "Knowing the directors who are active and productive in different regions can aid in recruiting top talent. This can lead to better quality productions and innovative content creation.\n",
        "\n",
        "• **Strategic Marketing:**\n",
        "\n",
        "By understanding the geographic distribution of directors, businesses can tailor their marketing strategies to target regions with high film production activity, thereby increasing the likelihood of engaging with relevant audiences.\n",
        "Potential Negative Growth\n",
        "\n",
        "• **Market Saturation:**\n",
        "\n",
        "If the data shows that certain markets are already saturated with prolific directors, entering these markets might be challenging. High competition can lead to increased costs and reduced chances of success.\n",
        "\n",
        "• **Resource Allocation:**\n",
        "\n",
        "Misinterpreting the data could lead to inefficient resource allocation. For example, investing heavily in a region with many directors but low market demand might not yield the expected returns.\n",
        "\n",
        "**Cultural Differences:**\n",
        "\n",
        "While the chart shows the number of directors per country, it doesn't account for cultural preferences and differences. Investing in a region without understanding the local audience's taste might lead to projects that don't resonate well, impacting growth negatively.\n",
        "\n",
        "***Justification***\n",
        "The insights from the chart provide a clear understanding of where productive directors are located and how they are distributed across different countries. This can help businesses make informed decisions regarding where to invest, who to collaborate with, and how to strategize their market entry and expansion. However, without careful analysis and consideration of market saturation, cultural differences, and proper resource allocation, there can be risks leading to negative growth. Properly leveraging the insights requires a balanced approach, considering both the opportunities and potential pitfalls highlighted by the data."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "targets = [\n",
        "    \"Target 1\", \"Target 2\", \"Target 3\", \"Target 4\",\n",
        "    \"Target 5\", \"Target 6\", \"Target 7\", \"Target 8\",\n",
        "    \"Target 9\", \"Target 10\", \"Target 11\", \"Target 12\"\n",
        "]\n",
        "\n",
        "sensitivity = [0.3, 0.4, 0.3, 0.7, 0.5, 0.2, 0.3, 0.7, 0.5, 0.3, 0.4, 0.3]\n",
        "specificity = [0.6, 0.3, 0.7, 0.3, 0.5, 0.8, 0.6, 0.3, 0.5, 0.7, 0.3, 0.6]\n",
        "accuracy = [0.5, 0.2, 0.4, 0.2, 0.3, 0.3, 0.5, 0.2, 0.3, 0.4, 0.2, 0.5]\n",
        "precision = [0.5, 0.2, 0.4, 0.2, 0.3, 0.3, 0.5, 0.2, 0.3, 0.4, 0.2, 0.5]\n",
        "auc = [0.65, 0.6, 0.3, 0.3, 0.3, 0.4, 0.65, 0.6, 0.3, 0.3, 0.3, 0.4]\n",
        "\n",
        "x = np.arange(len(targets))  # the label locations\n",
        "width = 0.15  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "rects1 = ax.bar(x - 2*width, sensitivity, width, label='Sensitivity')\n",
        "rects2 = ax.bar(x - width, specificity, width, label='Specificity')\n",
        "rects3 = ax.bar(x, accuracy, width, label='Accuracy')\n",
        "rects4 = ax.bar(x + width, precision, width, label='Precision')\n",
        "rects5 = ax.bar(x + 2*width, auc, width, label='AUC')\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Targets')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Comparison of Various Metrics Across Targets')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(targets)\n",
        "ax.legend()\n",
        "\n",
        "# Adding values on top of bars\n",
        "def add_labels(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "add_labels(rects1)\n",
        "add_labels(rects2)\n",
        "add_labels(rects3)\n",
        "add_labels(rects4)\n",
        "add_labels(rects5)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison Across Multiple Metrics:**\n",
        "\n",
        "Your data includes multiple metrics (Sensitivity, Specificity, Accuracy, Precision, and AUC) for each target. A grouped bar chart allows for clear comparison across these metrics within each target.\n",
        "\n",
        "**Categorical Data Representation:**\n",
        "\n",
        "Bar charts are particularly effective for categorical data. In this case, each target is a category, and the grouped bars allow us to visualize the performance metrics side by side.\n",
        "\n",
        "**Clarity and Readability:**\n",
        "\n",
        "Grouped bar charts provide a straightforward way to compare multiple series of data. Each metric is represented by a different color, making it easy to distinguish between them.\n",
        "\n",
        "**Highlighting Differences and Trends:**\n",
        "\n",
        "This chart type makes it easier to spot differences and trends across the targets. For example, we can quickly see which target has the highest sensitivity, specificity, etc.\n",
        "\n",
        "**Adding Data Labels**:\n",
        "\n",
        "The chart allows for data labels to be added on top of the bars, making it easier to read the exact values without cluttering the graph.\n",
        "In summary, a grouped bar chart effectively showcases the multi-dimensional nature of your data, providing a clear and concise visualization that facilitates comparison and interpretation."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the grouped bar chart visualizing the metrics (Sensitivity, Specificity, Accuracy, Precision, and AUC) for each target, we can derive several insights:\n",
        "\n",
        "**Overall Performance:**\n",
        "\n",
        "Target 0 generally exhibits the highest metrics across Sensitivity, Specificity, Accuracy, Precision, and AUC, indicating it is the best-performing target among the three.\n",
        "Target 2 shows the lowest metrics across all categories, suggesting it is the weakest performing target.\n",
        "\n",
        "**Sensitivity:**\n",
        "\n",
        "All targets have relatively high Sensitivity values, but Target 0 has the highest Sensitivity, implying it is most effective at correctly identifying positive cases.\n",
        "\n",
        "**Specificity:**\n",
        "\n",
        "Specificity values are lower compared to Sensitivity across all targets, with Target 2 showing the lowest Specificity. This indicates that there are more false positives for Target 2.\n",
        "\n",
        "**Accuracy:**\n",
        "\n",
        "Accuracy follows a similar trend to Sensitivity, with Target 0 having the highest accuracy and Target 2 the lowest. This suggests that Target 0 is the most reliable overall.\n",
        "\n",
        "**Precision:**\n",
        "\n",
        "Precision values are lower for Targets 1 and 2 compared to Target 0, indicating that there are more false positives in these targets. Target 0 has the highest Precision, suggesting it has fewer false positives.\n",
        "\n",
        "**AUC:**\n",
        "\n",
        "The AUC (Area Under the Curve) values show that Target 0 has the best balance between Sensitivity and Specificity, followed by Target 1 and then Target 2.\n",
        "\n",
        "**Metric Correlations**:\n",
        "\n",
        "The trends in Accuracy, Sensitivity, and AUC are quite aligned, suggesting that higher Sensitivity generally correlates with higher Accuracy and AUC.\n",
        "\n",
        "**Relative Differences:**\n",
        "\n",
        "The chart reveals that while all targets perform decently, there is a clear performance gap between Target 0 and the other two targets, especially Target 2.\n",
        "In summary, Target 0 is the best-performing target across all metrics, indicating it is the most reliable for the given context. Conversely, Target 2's lower performance across all metrics highlights areas where improvements might be necessary."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Positive Business Impact***\n",
        "The insights gained from the grouped bar chart can certainly help in creating a positive business impact in the following ways:\n",
        "\n",
        "**Resource Allocation:**\n",
        "\n",
        "By identifying Target 0 as the best-performing target, resources such as time, funding, and manpower can be focused on this target to maximize positive outcomes. This focus can improve efficiency and effectiveness, leading to better overall results.\n",
        "\n",
        "**Strategy Optimization:**\n",
        "\n",
        "Knowing that Target 2 has the lowest performance across all metrics allows for a strategic review. Efforts can be made to improve the processes and methods applied to Target 2. This could involve better training, improved tools, or revised procedures.\n",
        "\n",
        "**Risk Management:**\n",
        "\n",
        "The high performance of Target 0 means less risk associated with projects or products related to this target. This can give stakeholders more confidence and potentially attract more investment or interest.\n",
        "\n",
        "**Customer Satisfaction:**\n",
        "\n",
        "High-performing targets typically translate to better service or product quality. Focusing on Target 0 can lead to higher customer satisfaction and loyalty, driving repeat business and positive word-of-mouth.\n",
        "\n",
        "***Potential for Negative Growth***\n",
        "The insights also highlight areas that could potentially lead to negative growth if not addressed:\n",
        "\n",
        "**Target 2's Low Performance:**\n",
        "\n",
        "If the issues with Target 2 are not addressed, it could lead to increased costs due to inefficiencies and higher error rates. This can result in customer dissatisfaction, negative reviews, and ultimately a loss of business.\n",
        "\n",
        "**Imbalance in Resource Allocation:**\n",
        "\n",
        "While focusing on Target 0 can lead to positive outcomes, neglecting Target 2 could create an imbalance. This might result in long-term negative growth if Target 2 represents a significant portion of the business's market or customer base.\n",
        "\n",
        "**Reputational Risk:**\n",
        "\n",
        "Poor performance in any target area can harm the company’s reputation. If Target 2 continues to underperform, it could lead to negative perceptions about the company's overall capabilities, affecting brand image and customer trust.\n",
        "\n",
        "***Justification***\n",
        "**Resource Allocation:** Efficiently allocating resources to high-performing targets can drive growth by maximizing returns on investment. Conversely, ignoring underperforming areas can cause resource wastage and missed opportunities.\n",
        "\n",
        "**Strategy Optimization:** Continuous improvement in weaker areas ensures balanced growth and mitigates the risk of any single area dragging down overall performance.\n",
        "\n",
        "**Risk Management:** Focusing on reliable targets minimizes risk but ignoring the need for improvement in weaker areas can lead to vulnerabilities that competitors might exploit.\n",
        "\n",
        "**Customer Satisfaction:** High performance in specific targets ensures quality, but consistent underperformance in others can lead to dissatisfaction and churn, affecting long-term growth.\n",
        "\n",
        "In summary, the insights from the chart provide a clear direction for enhancing strengths and addressing weaknesses, which is essential for sustainable business growth. Neglecting the insights, particularly the need to improve Target 2, could lead to negative impacts over time."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Metric': ['Metric 1', 'Metric 1', 'Metric 1', 'Metric 2', 'Metric 2', 'Metric 2', 'Metric 3', 'Metric 3', 'Metric 3'],\n",
        "    'Target': ['Target 0', 'Target 1', 'Target 2', 'Target 0', 'Target 1', 'Target 2', 'Target 0', 'Target 1', 'Target 2'],\n",
        "    'Values': [85, 78, 65, 90, 80, 70, 88, 85, 75]\n",
        "}\n",
        "\n",
        "# Creating DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Pivoting the data\n",
        "pivot_df = df.pivot(index='Metric', columns='Target', values='Values')\n",
        "\n",
        "# Plotting the stacked bar chart\n",
        "pivot_df.plot(kind='bar', stacked=True)\n",
        "\n",
        "plt.title('Stacked Bar Chart for Metrics and Targets')\n",
        "plt.xlabel('Metric')\n",
        "plt.ylabel('Values')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Target')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear Comparison: Stacked bar charts are excellent for comparing the total and segment distribution of different categories. In this case, it allows us to compare the 'Values' for each 'Metric' and see how they are distributed among different 'Targets'.\n",
        "\n",
        "Categorical Data Representation: The data provided includes categorical variables ('Metric' and 'Target') with corresponding numerical values. Stacked bar charts effectively represent such data, providing a visual breakdown of the categories within each group.\n",
        "\n",
        "Insightful Segmentation: By stacking the bars, we can easily see not only the total values for each 'Metric' but also how these totals are divided among the different 'Targets'. This helps in identifying patterns or trends within each metric.\n",
        "\n",
        "Visual Clarity: Stacked bar charts offer a clear and concise way to present data that needs to show parts of a whole. It ensures that each segment is distinctly visible, making it easier to interpret the contribution of each 'Target' to the overall 'Metric' value.\n",
        "\n",
        "These reasons make the stacked bar chart an appropriate and effective choice for visualizing the given dataset."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target Contributions: Each metric's total value is composed of contributions from different targets. This segmentation helps identify which targets are significant contributors to each metric.\n",
        "\n",
        "Dominant Targets: For certain metrics, one target may dominate, indicating a higher influence or performance. For instance, if 'Target A' has a larger segment in 'Metric 1', it shows that 'Target A' is a major contributor to 'Metric 1'.\n",
        "\n",
        "Comparative Analysis: The chart allows for comparing metrics to see which have higher or lower overall values. This helps in identifying which metrics are performing well and which may need improvement.\n",
        "\n",
        "Trend Identification: By examining the distribution of targets across metrics, it’s possible to identify trends. For example, if 'Target B' consistently has low values across all metrics, it might indicate underperformance.\n",
        "\n",
        "Resource Allocation: Understanding which targets contribute the most to each metric can help in making informed decisions regarding resource allocation and strategic focus.\n",
        "\n",
        "Anomalies and Outliers: Any unexpected values or disproportionate segments can highlight anomalies or outliers that might require further investigation.\n",
        "\n",
        "These insights collectively help in understanding the performance and contributions of different targets towards each metric, facilitating better decision-making and strategy formulation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact\n",
        "Targeted Improvement: Identifying which targets contribute significantly to key metrics allows businesses to focus their efforts on enhancing those targets, leading to better overall performance.\n",
        "\n",
        "Resource Optimization: By understanding the contributions of different targets, businesses can allocate resources more efficiently, focusing on high-impact areas and potentially reducing costs in lower-impact areas.\n",
        "\n",
        "Strategic Planning: Insights into trends and dominant targets help in strategic planning and decision-making. Businesses can develop tailored strategies for each target, improving overall effectiveness and results.\n",
        "\n",
        "Performance Monitoring: The chart provides a clear view of how different targets are performing relative to each metric, facilitating continuous monitoring and quick adjustments as needed.\n",
        "\n",
        "Potential for Negative Growth\n",
        "Over-Reliance on Dominant Targets: If a business focuses too heavily on targets that are currently performing well, it may neglect other areas that could be developed for future growth. This could lead to missed opportunities and long-term negative impacts.\n",
        "\n",
        "Neglecting Low-Performing Targets: Conversely, focusing only on improving low-performing targets without understanding the reasons behind their performance can lead to wasted resources and effort. If the underlying issues are not addressed, these targets may continue to underperform, impacting overall growth.\n",
        "\n",
        "Misinterpretation of Data: Incorrectly interpreting the contributions of different targets could lead to poor decision-making. For example, a target with high contributions to a metric might be performing well due to external factors rather than internal excellence. Misunderstanding these nuances can result in ineffective strategies.\n",
        "\n",
        "Justification\n",
        "Balanced Focus: Ensuring that the business does not overly rely on a few high-performing targets while also not disproportionately investing in low-performing ones is crucial. Balanced focus and strategic investments based on a comprehensive understanding of the data can drive sustainable growth.\n",
        "\n",
        "Holistic View: The insights gained from the chart should be considered as part of a broader analysis, taking into account external factors, historical trends, and qualitative data. This holistic approach helps mitigate the risk of negative growth due to misinterpretation or overemphasis on certain targets.\n",
        "\n",
        "In summary, the insights from the chart can create a positive business impact if used wisely and in conjunction with other analyses. However, there is a risk of negative growth if the data is misinterpreted or if the business focuses too narrowly on certain targets without considering the bigger picture.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "# df = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with your actual file path\n",
        "\n",
        "# For demonstration purposes, let's create a sample DataFrame\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [5, 4, 3, 2, 1],\n",
        "    'feature3': [2, 3, 4, 5, 6],\n",
        "    'feature4': [5, 3, 1, 4, 2]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "\n",
        "# Title and labels\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Features')\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Relationship Overview:\n",
        "\n",
        "A correlation heatmap provides a comprehensive overview of the relationships between multiple numerical features in a dataset. It allows you to quickly see which features are positively or negatively correlated and the strength of these correlations.\n",
        "Identification of Patterns:\n",
        "\n",
        "The heatmap helps in identifying patterns and dependencies among variables. By visualizing the correlation matrix, you can easily spot any strong correlations that might indicate redundancy, multicollinearity, or other significant relationships.\n",
        "Data Reduction:\n",
        "\n",
        "For feature selection and dimensionality reduction, the correlation heatmap is valuable. It helps in identifying highly correlated features where one feature can potentially be dropped without significant loss of information, aiding in simplifying models.\n",
        "Ease of Interpretation:\n",
        "\n",
        "The visual representation is easy to interpret. Color gradients make it straightforward to distinguish between strong, moderate, and weak correlations, facilitating quicker decision-making.\n",
        "Anomaly Detection:\n",
        "\n",
        "It can also help in detecting anomalies or unexpected relationships in the data that might warrant further investigation.\n",
        "Overall, a correlation heatmap is a versatile and informative visualization tool that provides valuable insights into the structure and relationships within your dataset."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting insights from a correlation heatmap involves understanding how variables relate to each other:\n",
        "\n",
        "Positive Correlation: If two variables have a positive correlation (closer to 1), it means they tend to increase or decrease together. In the context of your data (if numeric variables were present), a positive correlation between, say, \"release_year\" and \"duration\" might indicate that newer movies tend to have longer durations.\n",
        "\n",
        "Negative Correlation: A negative correlation (closer to -1) suggests that as one variable increases, the other tends to decrease. For example, there might be a negative correlation between \"release_year\" and \"rating\", indicating that older movies tend to have different ratings compared to newer ones.\n",
        "\n",
        "No Correlation: A correlation close to 0 indicates no linear relationship between variables. For instance, \"release_year\" and \"title\" might have very little correlation, as the title of a movie is not directly related to its release year numerically.\n",
        "\n",
        "Insights Specific to Your Data: Without actual numeric data in the example provided, the insights are hypothetical. In a real dataset, you would analyze correlations specific to your variables. For instance, understanding which features (like duration, release year, rating) are closely correlated can help in understanding patterns or trends in your dataset.\n",
        "\n",
        "To extract meaningful insights from your correlation heatmap, look for strong positive or negative correlations. These can suggest which variables might influence each other and how changes in one variable may affect another. Always consider the context of your data and domain knowledge to interpret correlations correctly."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Importing necessary libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame creation\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [1, 3, 5, 3, 1],\n",
        "    'D': [2, 4, 2, 4, 2]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Creating pair plot\n",
        "sns.pairplot(df)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Overview: A pair plot provides a grid of plots showing the pairwise relationships between variables. This offers a holistic view of how variables interact with each other.\n",
        "\n",
        "Correlation Analysis: By looking at the scatter plots, you can quickly identify which variables are positively or negatively correlated, or if there is no correlation at all.\n",
        "\n",
        "Distribution Insights: The diagonal plots in a pair plot typically show the distribution of each variable. This allows you to understand the spread and skewness of each variable individually.\n",
        "\n",
        "Trend Identification: Patterns, clusters, and trends in the data become more apparent when viewed in a pair plot, making it easier to identify underlying relationships that might not be obvious with single plots.\n",
        "\n",
        "Data Exploration: During exploratory data analysis (EDA), pair plots help in identifying potential outliers, anomalies, or interesting patterns that warrant further investigation.\n",
        "\n",
        "Overall, a pair plot is a powerful visualization tool for understanding the complex relationships within a dataset and is highly useful in the initial stages of data analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Between Variables:\n",
        "\n",
        "Positive Correlation: When one variable increases as the other increases, it suggests a positive linear relationship.\n",
        "Negative Correlation: When one variable decreases as the other increases, it suggests a negative linear relationship.\n",
        "No Correlation: No clear pattern, indicating no linear relationship between the variables.\n",
        "Distribution of Variables:\n",
        "\n",
        "Skewness: If the distribution of a variable is skewed to the left or right.\n",
        "Kurtosis: If the distribution has heavy tails or is flat.\n",
        "Clusters:\n",
        "\n",
        "Identification of clusters within the data, which could indicate subgroups or categories within the dataset.\n",
        "Outliers:\n",
        "\n",
        "Detection of outliers that deviate significantly from the other data points.\n",
        "Non-Linear Relationships:\n",
        "\n",
        "Recognition of non-linear relationships that might not be apparent with other visualization techniques.\n",
        "Patterns and Trends:\n",
        "\n",
        "Patterns and trends that could suggest seasonality, cycles, or other repeating patterns in the data.\n",
        "Anomalies:\n",
        "\n",
        "Identification of anomalies or unusual data points that may need further investigation.\n",
        "Multivariate Relationships:\n",
        "\n",
        "Understanding how multiple variables interact with each other simultaneously, providing a more comprehensive view of the data.\n",
        "In summary, a pair plot helps in identifying correlations, distributions, clusters, outliers, patterns, and trends, offering a detailed view of the relationships between multiple variables in a dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1 (H1): Movies with higher budgets tend to have higher gross revenue\n",
        "Step 1: Formulate Hypotheses\n",
        "Null Hypothesis (H0): There is no correlation between budget and gross revenue.\n",
        "Alternative Hypothesis (H1): There is a positive correlation between budget and gross revenue.\n",
        "Step 2: Perform Hypothesis Testing\n",
        "Let's use Pearson's correlation coefficient to test this hypothesis.\n",
        "Hypothesis 2 (H2): There is a significant difference in average gross revenue between movies directed by Christopher Nolan and Steven Spielberg\n",
        "Step 1: Formulate Hypotheses\n",
        "Null Hypothesis (H0): The average gross revenue of movies directed by Christopher Nolan is equal to that of movies directed by Steven Spielberg.\n",
        "Alternative Hypothesis (H1): The average gross revenue of movies directed by Christopher Nolan is different from that of movies directed by Steven Spielberg.\n",
        "Step 2: Perform Hypothesis Testing\n",
        "Let's use an independent t-test to test this hypothesis.\n",
        "Hypothesis 3 (H3): Movies with a higher IMDb rating have a longer duration\n",
        "Step 1: Formulate Hypotheses\n",
        "Null Hypothesis (H0): There is no correlation between IMDb rating and duration.\n",
        "Alternative Hypothesis (H1): There is a positive correlation between IMDb rating and duration.\n",
        "Step 2: Perform Hypothesis Testing\n",
        "Let's use Pearson's correlation coefficient to test this hypothesis."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the average revenue generated by movies directed by Christopher Nolan compared to movies directed by Steven Spielberg.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant difference in the average revenue generated by movies directed by Christopher Nolan compared to movies directed by Steven Spielberg.\n",
        "\n",
        "This hypothesis will guide us in conducting statistical tests to determine if there exists a significant difference in revenue between movies directed by Christopher Nolan and those directed by Steven Spielberg.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Generate hypothetical data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "\n",
        "# Assume gross revenues (in millions) for movies by Christopher Nolan\n",
        "nolan_revenues = np.random.normal(loc=150, scale=30, size=30)  # Mean 150, SD 30, 30 samples\n",
        "\n",
        "# Assume gross revenues (in millions) for movies by Steven Spielberg\n",
        "spielberg_revenues = np.random.normal(loc=140, scale=25, size=30)  # Mean 140, SD 25, 30 samples\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_statistic, p_value = stats.ttest_ind(nolan_revenues, spielberg_revenues)\n",
        "\n",
        "# Print the results\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in average gross revenue.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in average gross revenue.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine if there is a significant difference in the average revenue generated by movies directed by Christopher Nolan compared to movies directed by Steven Spielberg, you can perform an independent samples t-test. This test is appropriate when comparing the means of two independent groups (in this case, movies directed by Nolan vs. Spielberg) to assess whether there is evidence to reject the null hypothesis of equal means.\n",
        "\n",
        "Here's a step-by-step outline of how you can perform this test:\n",
        "\n",
        "Define Hypotheses:\n",
        "\n",
        "Null Hypothesis (H0): There is no significant difference in the average revenue between movies directed by Christopher Nolan and movies directed by Steven Spielberg.\n",
        "Alternative Hypothesis (H1): There is a significant difference in the average revenue between movies directed by Christopher Nolan and movies directed by Steven Spielberg.\n",
        "Collect Data:\n",
        "\n",
        "Gather revenue data for movies directed by Christopher Nolan and movies directed by Steven Spielberg.\n",
        "Assumptions:\n",
        "\n",
        "Independent samples: The revenue data for movies directed by Nolan and Spielberg are independent of each other.\n",
        "Normality: Each group's revenue data should be approximately normally distributed.\n",
        "Equal variance: The variances of the two groups (Nolan's movies and Spielberg's movies) should be equal.\n",
        "Perform the t-test:\n",
        "\n",
        "Calculate the t-statistic and corresponding p-value using statistical software or programming languages like Python (using libraries such as scipy.stats).\n",
        "Interpret Results:\n",
        "\n",
        "If the p-value is less than a chosen significance level (commonly 0.05), you reject the null hypothesis, indicating that there is a significant difference in average revenue between movies directed by Nolan and Spielberg.\n",
        "If the p-value is greater than the significance level, you fail to reject the null hypothesis, suggesting no significant difference in average revenue between the two directors' movies.\n",
        "Let me know if you need assistance with the actual implementation of this test in Python or any other statistical details!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I recommended the independent samples t-test for hypothesis test 1 (comparing the average revenue of movies directed by Christopher Nolan and Steven Spielberg) for several reasons:\n",
        "\n",
        "Comparison of Means: The t-test is suitable when comparing the means of two independent groups, which aligns perfectly with our scenario of comparing revenue between two different directors' movies.\n",
        "\n",
        "Assumption of Normality: While it's ideal for the data to be normally distributed within each group, the t-test is robust against moderate departures from normality, especially with larger sample sizes. This assumption is generally reasonable for revenue data.\n",
        "\n",
        "Assumption of Equal Variances: The t-test assumes that the variances of the two groups (movies directed by Nolan and Spielberg) are equal. This assumption can be checked using statistical tests like Levene's test or by visual inspection of the data.\n",
        "\n",
        "Interpretability: The t-test provides a straightforward interpretation of results, specifically whether there is a statistically significant difference in means between the two groups (directors' movies).\n",
        "\n",
        "Widely Accepted: The t-test is a widely used and accepted method for comparing means in statistical analysis, making it appropriate for hypothesis testing in many research contexts.\n",
        "\n",
        "If you have any specific concerns or considerations regarding the assumptions or applicability of the t-test to your dataset, feel free to ask for further clarification or assistance!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Hypothesis (Hypothesis Statement 2):\n",
        "There is a significant difference in the average IMDb ratings of movies directed by Christopher Nolan and Steven Spielberg.\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the average IMDb ratings of movies directed by Christopher Nolan and Steven Spielberg.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average IMDb ratings of movies directed by Christopher Nolan and Steven Spielberg.\n",
        "\n",
        "These hypotheses suggest that we are testing whether there is a statistically significant difference in IMDb ratings between movies directed by Nolan and Spielberg."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "\n",
        "# Example data (replace with actual box office revenue data)\n",
        "action_movies = [10000000, 15000000, 12000000, 18000000, 9000000]\n",
        "comedy_movies = [8000000, 9500000, 11000000, 8500000, 10500000]\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(action_movies, comedy_movies)\n",
        "\n",
        "# Print results\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in box office revenues between action and comedy movies.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in box office revenues between action and comedy movies.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For hypothesis statement 2, which involves comparing the mean box office revenues of action movies and comedy movies, the appropriate statistical test used to obtain the p-value is the two-sample t-test. This test is chosen because it allows us to compare the means of two independent groups (in this case, action movies and comedy movies) to determine if there is a statistically significant difference between their average box office revenues."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of Means: The hypothesis involves comparing the mean box office revenues of two independent groups (action movies and comedy movies). The two-sample t-test is specifically designed for comparing means between two groups.\n",
        "\n",
        "Assumption of Normality: The t-test assumes that the data within each group (box office revenues of action and comedy movies) are approximately normally distributed. This assumption is reasonable for many types of continuous data, such as financial metrics like box office revenues.\n",
        "\n",
        "Independence: The t-test assumes that the observations within each group are independent of each other, which is typically the case in movie box office data where each movie's performance is considered independently of others.\n",
        "\n",
        "Parametric Test: The t-test is a parametric test that provides a robust way to test differences between means when the data meet the assumptions. It is sensitive to differences in means and widely used for comparing continuous variables.\n",
        "\n",
        "Given these reasons, the two-sample t-test is appropriate for hypothesis 2 to determine if there is a statistically significant difference in mean box office revenues between action and comedy movies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): There is no significant difference in audience ratings between movies directed by male directors and movies directed by female directors.\n",
        "\n",
        "Alternative Hypothesis (H₁): There is a significant difference in audience ratings between movies directed by male directors and movies directed by female directors.\n",
        "\n",
        "This hypothesis aims to explore if there's a statistically significant disparity in audience ratings based on the gender of the movie directors."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming sales data for Product A and Product B are stored in arrays or DataFrames\n",
        "# Replace 'data_product_a' and 'data_product_b' with your actual data\n",
        "\n",
        "# Example data (replace with your actual data)\n",
        "data_product_a = [10, 12, 15, 8, 11]\n",
        "data_product_b = [13, 16, 14, 9, 12]\n",
        "\n",
        "# Perform independent t-test\n",
        "t_statistic, p_value = stats.ttest_ind(data_product_a, data_product_b)\n",
        "\n",
        "# Output the results\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in mean sales.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in mean sales.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing Means (Continuous Data):\n",
        "\n",
        "Student's t-test: Used to compare the means of two groups.\n",
        "ANOVA (Analysis of Variance): Used to compare means of more than two groups.\n",
        "Paired t-test: Used when comparing means of the same group under different conditions.\n",
        "Comparing Proportions (Categorical Data):\n",
        "\n",
        "Chi-square test: Used to determine if there is a significant association between categorical variables.\n",
        "Fisher's exact test: Similar to the chi-square test but used when sample sizes are small.\n",
        "Regression Analysis:\n",
        "\n",
        "Linear regression: Used to assess the relationship between one dependent (continuous) variable and one or more independent variables.\n",
        "Logistic regression: Used when the dependent variable is categorical (binary or multinomial).\n",
        "Non-parametric Tests:\n",
        "\n",
        "Mann-Whitney U test: Non-parametric alternative to the t-test for comparing two independent groups.\n",
        "Kruskal-Wallis test: Non-parametric alternative to ANOVA for comparing more than two independent groups.\n",
        "Choosing the Test:\n",
        "Nature of Data: Determine if your data is continuous or categorical.\n",
        "Number of Groups: Decide if you are comparing two groups, more than two groups, or multiple variables simultaneously.\n",
        "Specific Hypothesis: Tailor the test to match the specific hypothesis statement and data characteristics."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If Hypothesis 3 involves comparing means:\n",
        "\n",
        "Scenario: You want to test if there is a significant difference in mean customer satisfaction scores between two different service models (continuous data).\n",
        "Statistical Test: Use a two-sample t-test if comparing two groups, or ANOVA if comparing more than two groups.\n",
        "If Hypothesis 3 involves comparing proportions:\n",
        "\n",
        "Scenario: You want to determine if there is a significant difference in the proportion of customers who prefer product A versus product B (categorical data).\n",
        "Statistical Test: Chi-square test or Fisher's exact test could be appropriate depending on sample sizes and assumptions.\n",
        "If Hypothesis 3 involves relationship or correlation:\n",
        "\n",
        "Scenario: You want to examine if there is a significant linear relationship between advertising spending and sales revenue (continuous data).\n",
        "Statistical Test: Pearson correlation coefficient or linear regression analysis would be suitable."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Classroom/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Imputation\n",
        "\n",
        "Description: This technique involves replacing missing values with the mean (average) value of the observed data in the column.\n",
        "Use Case: Suitable for numerical data without significant outliers.\n",
        "Justification: It preserves the mean of the dataset and is simple to implement. However, it can distort the variance and correlation structures.\n",
        "Median Imputation\n",
        "\n",
        "Description: Missing values are replaced with the median value of the observed data in the column.\n",
        "Use Case: Preferred for numerical data with outliers or skewed distributions.\n",
        "Justification: Median imputation is robust to outliers and maintains the central tendency without being influenced by extreme values.\n",
        "Most Frequent Imputation\n",
        "\n",
        "Description: This method replaces missing values with the most frequently occurring value (mode) in the column.\n",
        "Use Case: Useful for both numerical and categorical data where a single value dominates.\n",
        "Justification: It is effective in maintaining the mode of the dataset, especially for categorical features.\n",
        "Constant Value Imputation\n",
        "\n",
        "Description: Missing values are replaced with a specified constant value, such as zero or a placeholder category.\n",
        "Use Case: When there is a meaningful constant value that can be used, such as zero in financial datasets or a specific category in categorical data.\n",
        "Justification: It ensures that all missing values are filled with a contextually appropriate constant, avoiding the introduction of biases from statistical measures like mean or median.\n",
        "Forward Fill and Backward Fill\n",
        "\n",
        "Description: Missing values are filled using the previous or next observed value in the column, respectively.\n",
        "Use Case: Time-series data where the assumption is that the missing value is similar to the previous or next value.\n",
        "Justification: Preserves the temporal structure of the data, which is crucial in time-series analysis.\n",
        "Interpolation\n",
        "\n",
        "Description: Estimates missing values by interpolating between the known values before and after the missing value.\n",
        "Use Case: Suitable for numerical time-series data.\n",
        "Justification: Provides a smooth transition between data points, maintaining the overall trend and pattern in the data.\n",
        "K-Nearest Neighbors (KNN) Imputation\n",
        "\n",
        "Description: Uses the k-nearest neighbors algorithm to impute missing values based on the similarity of other observations.\n",
        "Use Case: Numerical and categorical data where the assumption is that similar data points have similar values.\n",
        "Justification: Captures the underlying patterns and correlations in the data, leading to more accurate imputations.\n",
        "Regression Imputation\n",
        "\n",
        "Description: Predicts missing values using a regression model based on other variables in the dataset.\n",
        "Use Case: When there is a strong correlation between the feature with missing values and other features.\n",
        "Justification: Leverages relationships between variables to provide accurate estimates of missing values.\n",
        "Multiple Imputation\n",
        "\n",
        "Description: Creates multiple imputed datasets by incorporating random variation, then combines the results.\n",
        "Use Case: When the dataset has a high proportion of missing values and a single imputation may introduce bias.\n",
        "Justification: Accounts for the uncertainty of missing data and provides more robust and reliable estimates.\n",
        "Summary\n",
        "Mean Imputation: Simple and maintains the overall average but can distort variance.\n",
        "Median Imputation: Robust to outliers and skewed data.\n",
        "Most Frequent Imputation: Effective for categorical data and preserves mode.\n",
        "Constant Value Imputation: Useful when there is a meaningful constant to replace missing values.\n",
        "Forward Fill and Backward Fill: Maintains temporal structure in time-series data.\n",
        "Interpolation: Smooth transition for time-series data.\n",
        "K-Nearest Neighbors Imputation: Captures underlying data patterns.\n",
        "Regression Imputation: Uses correlations for accurate estimates.\n",
        "Multiple Imputation: Handles uncertainty and provides robust estimates.\n",
        "Selecting the appropriate imputation technique depends on the nature of the data, the extent of missing values, and the analysis goals. Each technique has its strengths and limitations, and the choice should be guided by the specific context and characteristics of the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Sample DataFrame for demonstration purposes\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100],\n",
        "    'B': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 200]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Removing Outliers\n",
        "Technique: Directly removing the rows that contain outliers.\n",
        "\n",
        "Reason: This technique is useful when outliers are likely to be data errors or anomalies that do not represent the underlying data distribution. By removing them, we ensure that these anomalies do not skew our analysis.\n",
        "\n",
        "2. Capping (Winsorizing)\n",
        "Technique: Limiting extreme values at specified percentiles.\n",
        "\n",
        "Reason: This method is used when we want to reduce the impact of extreme values without completely removing them. Capping ensures that extreme outliers are brought within a certain range, thus minimizing their influence on statistical measures like mean and variance.\n",
        "\n",
        "3. Transformation\n",
        "Technique: Applying a mathematical transformation, such as log transformation, to reduce skewness.\n",
        "\n",
        "Reason: Transformations can help make the data more normally distributed, especially when the data is positively skewed. This is particularly useful for techniques that assume normality, such as certain regression models and hypothesis tests.\n",
        "\n",
        "4. Imputation\n",
        "Technique: Replacing outliers with a central tendency measure, such as the median.\n",
        "\n",
        "Reason: This method retains all data points but reduces the influence of extreme values by replacing them with a less extreme value (e.g., median). This is useful when outliers are legitimate values but still disproportionately affect the analysis.\n",
        "\n",
        "5. Clustering Methods (DBSCAN)\n",
        "Technique: Using clustering algorithms like DBSCAN to identify outliers as points that do not belong to any cluster.\n",
        "\n",
        "Reason: Clustering methods help in identifying outliers based on the distribution and density of the data. This technique is effective when outliers do not fit well into the overall data pattern. DBSCAN, in particular, can identify outliers without making any assumptions about the distribution of data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Sample DataFrame with categorical columns\n",
        "data = {\n",
        "    'Category': ['A', 'B', 'C', 'A', 'C'],\n",
        "    'Status': ['Active', 'Inactive', 'Active', 'Active', 'Inactive']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original DataFrame:\\n\", df)\n",
        "\n",
        "# Perform Label Encoding on 'Category'\n",
        "label_encoder = LabelEncoder()\n",
        "df['Category_LabelEncoded'] = label_encoder.fit_transform(df['Category'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In data preprocessing, various categorical encoding techniques are used to convert categorical variables into numerical representations that machine learning algorithms can process effectively. Here are some commonly used techniques and their rationales:\n",
        "\n",
        "Label Encoding:\n",
        "\n",
        "Technique: Assigns a unique integer to each category in a categorical variable.\n",
        "Rationale: Suitable for ordinal data where there is an inherent order among categories (e.g., low, medium, high). Helps in preserving ordinal relationships.\n",
        "One-Hot Encoding:\n",
        "\n",
        "Technique: Creates binary columns for each category and assigns a 1 or 0 (True/False) to indicate the presence of a category in each observation.\n",
        "Rationale: Ideal for nominal data without an inherent order (e.g., colors, countries). Prevents ordinal relationships from being inferred and avoids bias in models.\n",
        "Dummy Encoding:\n",
        "\n",
        "Technique: Similar to One-Hot Encoding but drops one of the binary columns to avoid multicollinearity in linear models.\n",
        "Rationale: Useful when using linear models where multicollinearity (high correlation among predictors) can affect model performance.\n",
        "Effect Encoding:\n",
        "\n",
        "Technique: Represents each level of a categorical variable relative to a chosen reference level.\n",
        "Rationale: Useful in regression models where you want to interpret coefficients relative to a baseline level. It can handle multicollinearity and provide meaningful interpretation.\n",
        "Binary Encoding:\n",
        "\n",
        "Technique: Converts each category into binary code, then splits the binary digits into separate columns.\n",
        "Rationale: Reduces the number of columns compared to One-Hot Encoding while still capturing the uniqueness of each category. It's efficient for high-cardinality categorical variables.\n",
        "Hashing Encoding:\n",
        "\n",
        "Technique: Hashes categorical values into a specified number of bins and assigns each category to a bin.\n",
        "Rationale: Useful when dealing with very large categorical variables to reduce memory usage and dimensionality.\n",
        "Selection Criteria:\n",
        "Nature of Data: Choose based on whether the categorical variable is ordinal or nominal.\n",
        "Model Requirements: Consider the model's sensitivity to encoding methods (e.g., linear models and multicollinearity).\n",
        "Performance: Evaluate encoding techniques based on how they impact model performance, especially in terms of accuracy, interpretability, and computational efficiency.\n",
        "By understanding the nature of your categorical data and the requirements of your machine learning model, you can select the most appropriate encoding technique to preprocess your data effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define a mapping dictionary for common English contractions\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions in a text using the mapping dictionary\n",
        "def expand_contractions(text, contraction_mapping):\n",
        "    \"\"\"\n",
        "    Expand contractions in a piece of text using a mapping dictionary.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text containing contractions.\n",
        "    - contraction_mapping (dict): Mapping dictionary for expanding contractions.\n",
        "\n",
        "    Returns:\n",
        "    - str: Text with expanded contractions.\n",
        "    \"\"\"\n",
        "    # Regular expression pattern to find contractions\n",
        "    contraction_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "                                     flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    def expand_match(contraction):\n",
        "        \"\"\"\n",
        "        Function to expand a single contraction match using the mapping dictionary.\n",
        "\n",
        "        Args:\n",
        "        - contraction (str): Single contraction match.\n",
        "\n",
        "        Returns:\n",
        "        - str: Expanded form of the contraction.\n",
        "        \"\"\"\n",
        "        match = contraction.group(0)\n",
        "        expanded_contraction = contraction_mapping.get(match) \\\n",
        "            if contraction_mapping.get(match) \\\n",
        "            else contraction_mapping.get(match.lower())\n",
        "        return expanded_contraction\n",
        "\n",
        "    # Replace contractions in text using the expand_match function\n",
        "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
        "    return expanded_text\n",
        "\n",
        "# Example usage\n",
        "text_with_contractions = \"I can't believe we've made it!\"\n",
        "expanded_text = expand_contractions(text_with_contractions, contraction_mapping)\n",
        "print(\"Original Text:\", text_with_contractions)\n",
        "print(\"Text after Expanding Contractions:\", expanded_text)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Example text with mixed cases\n",
        "text = \"Hello World! This Is a Sample Text With MIXED Cases.\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "lowercased_text = text.lower()\n",
        "\n",
        "# Print the original and lowercased text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lowercased Text:\", lowercased_text)\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import re\n",
        "\n",
        "# Example text with punctuations\n",
        "text = \"Hello, World! This is a sample text with punctuations.\"\n",
        "\n",
        "# Define a function to remove punctuations using regex\n",
        "def remove_punctuations(text):\n",
        "    # Define regex pattern for punctuations\n",
        "    pattern = r'[^\\w\\s]'  # Matches any character that is not alphanumeric or whitespace\n",
        "\n",
        "    # Use re.sub to substitute punctuations with an empty string\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Remove punctuations from the text\n",
        "clean_text = remove_punctuations(text)\n",
        "\n",
        "# Print the original and cleaned text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Text without Punctuations:\", clean_text)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Define the regex pattern for URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "    # Remove URLs from the text using the sub method\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "# Example usage\n",
        "text_with_urls = \"Check out this cool website: https://example.com. Also visit www.anotherexample.com\"\n",
        "clean_text = remove_urls(text_with_urls)\n",
        "print(\"Text with URLs:\", text_with_urls)\n",
        "print(\"Text without URLs:\", clean_text)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Load stopwords from NLTK\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the filtered words back into a single string\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "# Example usage\n",
        "text_with_stopwords = \"This is a sample sentence, demonstrating the removal of stopwords.\"\n",
        "clean_text = remove_stopwords(text_with_stopwords)\n",
        "print(\"Text with stopwords:\", text_with_stopwords)\n",
        "print(\"Text without stopwords:\", clean_text)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_whitespace(text):\n",
        "    # Remove leading and trailing white spaces\n",
        "    return text.strip()\n",
        "\n",
        "# Example usage\n",
        "text_with_whitespace = \"   This is a sample sentence with white spaces.    \"\n",
        "clean_text = remove_whitespace(text_with_whitespace)\n",
        "print(\"Text with white spaces:\", repr(text_with_whitespace))  # repr() to show white spaces\n",
        "print(\"Text without white spaces:\", repr(clean_text))\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "movies = [\n",
        "    {\"title\": \"3%\", \"description\": \"A dystopian future where the elite 3% escape poverty.\"},\n",
        "    {\"title\": \"7:19\", \"description\": \"Survivors trapped in Mexico City after an earthquake.\"},\n",
        "    {\"title\": \"23:59\", \"description\": \"Soldiers face supernatural forces on a jungle island.\"},\n",
        "    {\"title\": \"9\", \"description\": \"Rag-doll robots fight for survival in a post-apocalyptic world.\"},\n",
        "    {\"title\": \"21\", \"description\": \"Brilliant students become blackjack experts in Las Vegas.\"},\n",
        "    {\"title\": \"46\", \"description\": \"A genetics professor experiments to save his sister.\"},\n",
        "    {\"title\": \"122\", \"description\": \"A couple faces horror in a hospital after an accident.\"},\n",
        "    {\"title\": \"187\", \"description\": \"A teacher faces new challenges after leaving New York City.\"},\n",
        "    {\"title\": \"706\", \"description\": \"A psychiatrist investigates a psychic patient's condition.\"},\n",
        "    {\"title\": \"1920\", \"description\": \"Architect encounters supernatural forces in a castle.\"},\n",
        "    {\"title\": \"1922\", \"description\": \"A farmer's confession triggers horrific events in a town.\"},\n",
        "    {\"title\": \"1983\", \"description\": \"Law student and detective uncover a hidden conspiracy.\"},\n",
        "    {\"title\": \"1994\", \"description\": \"Examines Mexican politics during a pivotal year.\"},\n",
        "    {\"title\": \"2,215\", \"description\": \"Rock star's charity run across Thailand.\"},\n",
        "    {\"title\": \"3022\", \"description\": \"Astronauts battle isolation on a stranded space station.\"},\n",
        "    {\"title\": \"Oct-01\", \"description\": \"Murder investigation during Nigeria's struggle for independence.\"},\n",
        "    {\"title\": \"Feb-09\", \"description\": \"Family dynamics and Alzheimer's affect relationships.\"},\n",
        "    {\"title\": \"22-Jul\", \"description\": \"Norway's response to devastating terror attacks.\"},\n",
        "    {\"title\": \"15-Aug\", \"description\": \"Mumbai chawl's unity on India's Independence Day.\"},\n",
        "    {\"title\": \"89\", \"description\": \"Chronicles Arsenal's championship victory in 1989.\"},\n",
        "    {\"title\": \"Kuch Bheege Alfaaz\", \"description\": \"Two strangers form a deep online friendship.\"},\n",
        "    {\"title\": \"Goli Soda 2\", \"description\": \"Characters strive for better lives amid corruption.\"},\n",
        "    {\"title\": \"Maj Rati Keteki\", \"description\": \"Writer reunites with hometown, evoking memories.\"},\n",
        "    {\"title\": \"Mayurakshi\", \"description\": \"Middle-aged divorcee confronts past emotions.\"},\n",
        "    {\"title\": \"SAINT SEIYA: Knights of the Zodiac\", \"description\": \"Knights protect Athena amid prophecy.\"},\n",
        "    {\"title\": \"(T)ERROR\", \"description\": \"Real-life glimpse into FBI counterterrorism operations.\"},\n",
        "    {\"title\": \"(Un)Well\", \"description\": \"Explores commercialized promises in the wellness industry.\"},\n",
        "    {\"title\": \"#Alive\", \"description\": \"Surviving a zombie apocalypse in urban isolation.\"}\n",
        "]\n",
        "\n",
        "# Accessing each movie and its description\n",
        "for movie in movies:\n",
        "    print(f\"Title: {movie['title']}\")\n",
        "    print(f\"Description: {movie['description']}\")\n",
        "    print()  # Blank line for separation\n",
        "\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import spacy\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"I love programming. It's very fulfilling!\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Word tokenization\n",
        "word_tokens = [token.text for token in doc]\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "\n",
        "# Sentence tokenization\n",
        "sentence_tokens = [sent.text for sent in doc.sents]\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Sample words\n",
        "words = [\"running\", \"happily\", \"cats\", \"fishing\", \"fished\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided comprehensive example of text preprocessing, I primarily used lemmatization along with part-of-speech (POS) tagging. Here’s why lemmatization was chosen and the rationale behind the combination of techniques used:\n",
        "\n",
        "Why Lemmatization?\n",
        "Context-Awareness:\n",
        "\n",
        "Lemmatization reduces words to their base or dictionary form (lemma), considering the context provided by POS tags. For example, \"better\" is lemmatized to \"good\" when identified as an adjective, and \"running\" is lemmatized to \"run\" when identified as a verb.\n",
        "Accuracy:\n",
        "\n",
        "Lemmatization is generally more accurate than stemming because it produces valid words that retain their meaning. This is crucial for applications where understanding the semantics of the text is important, such as in sentiment analysis, machine translation, and text summarization.\n",
        "Why Part-of-Speech Tagging?\n",
        "Improved Lemmatization:\n",
        "POS tagging provides the necessary context to perform accurate lemmatization. Different forms of a word (e.g., \"run\" as a noun and \"run\" as a verb) are lemmatized correctly based on their POS tags.\n",
        "Other Techniques Used\n",
        "Text Cleaning:\n",
        "\n",
        "Lowercasing: Converts all text to lowercase to ensure uniformity.\n",
        "Removing Numbers and Punctuation: Simplifies the text and removes noise that might not contribute to the analysis.\n",
        "Removing Extra Whitespace: Ensures that the text is clean and uniformly spaced.\n",
        "Tokenization:\n",
        "\n",
        "Word Tokenization: Splits the text into individual words, which is a fundamental step before any further processing.\n",
        "Stop Word Removal:\n",
        "\n",
        "Removes common words that do not carry significant meaning, such as \"the\", \"is\", \"in\", etc., to reduce the dimensionality of the data and focus on more meaningful words.\n",
        "Summary\n",
        "Lemmatization was chosen for its accuracy and context-awareness, which are critical for understanding the semantics of text.\n",
        "POS Tagging was used to improve the accuracy of lemmatization by providing context.\n",
        "Other preprocessing steps like text cleaning, tokenization, and stop word removal were included to prepare the text comprehensively for further analysis and modeling.\n"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(word_tokens)\n",
        "\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"I love programming.\",\n",
        "    \"Programming is fun.\",\n",
        "    \"I love learning new things.\"\n",
        "]\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
        "print(\"BoW Vector:\\n\", X.toarray())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Bag of Words (BoW)\n",
        "Used For:\n",
        "\n",
        "Simpler NLP tasks where the context and semantics of words are less important, such as text classification with a small dataset.\n",
        "Initial feature extraction to get a quick overview of the most frequent words.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "Used For:\n",
        "\n",
        "Tasks where the relative importance of words matters, such as document classification and information retrieval.\n",
        "Improving upon BoW by reducing the weight of commonly occurring words\n",
        " Word Embeddings (Word2Vec)\n",
        "Used For:\n",
        "\n",
        "Capturing semantic relationships between words for tasks like word similarity, sentiment analysis, and more complex NLP applications.\n",
        "Sentence Embeddings (BERT)\n",
        "Used For:\n",
        "\n",
        "Tasks requiring context-aware understanding, such as text classification, question answering, and other advanced NLP applications.\n",
        "Summary of Choices\n",
        "In summary, the choice of text vectorization technique depends on the specific requirements of your NLP task:\n",
        "\n",
        "BoW and TF-IDF: Simple, interpretable, and computationally inexpensive. Suitable for basic text classification and retrieval tasks.\n",
        "Word Embeddings: Capture semantic relationships and are suitable for tasks requiring word-level understanding.\n",
        "Sentence Embeddings: Provide deep contextual understanding, ideal for complex tasks that need context-aware representations.\n",
        "By choosing the appropriate vectorization technique, you can better prepare your text data for subsequent machine learning or NLP tasks, ensuring that the models you build are as effective and accurate as possible."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': np.random.rand(100),\n",
        "    'Feature2': np.random.rand(100),\n",
        "    'Feature3': np.random.rand(100),\n",
        "    'Feature4': np.random.rand(100)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some correlation for demonstration\n",
        "df['Feature2'] = df['Feature1'] + np.random.normal(0, 0.1, 100)\n",
        "df['Feature3'] = df['Feature1'] * 2 + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Check for low variance features\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': np.random.rand(100),\n",
        "    'Feature2': np.random.rand(100),\n",
        "    'Feature3': np.random.rand(100),\n",
        "    'Feature4': np.random.rand(100)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some low variance for demonstration\n",
        "df['Feature5'] = 1  # Zero variance feature\n",
        "df['Feature6'] = df['Feature1'] + 1e-9 * np.random.rand(100)  # Very low variance\n",
        "\n",
        "# Apply Variance Threshold\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "selector.fit(df)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = df.columns[selector.get_support()]\n",
        "df_selected = df[selected_features]\n",
        "\n",
        "print(\"Selected Features:\", selected_features)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Variance Thresholding\n",
        "Why Used:\n",
        "\n",
        "To remove features with low variance that do not contribute much information to the model. Features with zero or near-zero variance can be considered uninformative as they do not vary much between different samples.\n",
        "This method is simple and quick to implement.\n",
        "2. Feature Importance from Models\n",
        "Why Used:\n",
        "\n",
        "Tree-based models (like Random Forest) can naturally provide feature importance scores based on how useful each feature is in reducing impurity.\n",
        "This method helps in identifying which features are most influential in making predictions.\n",
        "3. Recursive Feature Elimination (RFE)\n",
        "Why Used:\n",
        "\n",
        "RFE is a wrapper method that recursively removes the least important features based on the model’s performance, helping to identify a subset of features that contribute most to the model’s accuracy.\n",
        "This method is iterative and provides a ranking of features, making it more thorough\n",
        "4. Statistical Tests (Chi-Squared Test)\n",
        "Why Used:\n",
        "\n",
        "Statistical tests can identify which features have the strongest relationship with the target variable. Chi-squared tests are particularly useful for categorical features.\n",
        "This method is based on statistical significance, which can provide a quantitative basis for feature selection\n",
        "5. Principal Component Analysis (PCA)\n",
        "Why Used:\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms the features into a set of linearly uncorrelated components, preserving as much variance as possible.\n",
        "This method helps in reducing the dimensionality of the data while retaining most of the important information\n",
        "\n",
        "In the examples provided, several feature selection methods were demonstrated, each chosen for its specific advantages in reducing dimensionality, improving model performance, and mitigating overfitting. Here’s a summary of the methods used and the rationale behind their selection:\n",
        "\n",
        "1. Variance Thresholding\n",
        "Why Used:\n",
        "\n",
        "To remove features with low variance that do not contribute much information to the model. Features with zero or near-zero variance can be considered uninformative as they do not vary much between different samples.\n",
        "This method is simple and quick to implement.\n",
        "\n",
        "Tree-based models (like Random Forest) can naturally provide feature importance scores based on how useful each feature is in reducing impurity.\n",
        "This method helps in identifying which features are most influential in making predictions.\n",
        "\n",
        "RFE is a wrapper method that recursively removes the least important features based on the model’s performance, helping to identify a subset of features that contribute most to the model’s accuracy.\n",
        "This method is iterative and provides a ranking of features, making it more thorough.\n",
        "\n",
        "\n",
        "Statistical tests can identify which features have the strongest relationship with the target variable. Chi-squared tests are particularly useful for categorical features.\n",
        "This method is based on statistical significance, which can provide a quantitative basis for feature selection.\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms the features into a set of linearly uncorrelated components, preserving as much variance as possible.\n",
        "This method helps in reducing the dimensionality of the data while retaining most of the important information.\n",
        "\n",
        "Regularization methods like Lasso (L1) regression add a penalty for large coefficients and can help in feature selection by shrinking less important feature coefficients to zero.\n",
        "This method is effective in selecting a sparse set of features and reducing model complexity.\n",
        "Summary of Feature Selection Methods and Their Use Cases\n",
        "Variance Thresholding: Quickly remove features with little to no variance.\n",
        "Feature Importance from Models: Identify and prioritize influential features.\n",
        "Recursive Feature Elimination (RFE): Iteratively select features by recursively considering smaller sets.\n",
        "Statistical Tests: Select features based on statistical significance.\n",
        "Principal Component Analysis (PCA): Reduce dimensionality while preserving variance.\n",
        "Regularization (Lasso Regression): Select a sparse set of features by penalizing large coefficients."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Feature Importance from Tree-Based Models\n",
        "Tree-based models like Random Forest or Gradient Boosting Machines (GBM) provide a feature importance score based on how much each feature contributes to reducing the impurity in the nodes of the trees. Features with higher importance scores are considered more influential in making predictions.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "If a Random Forest model indicates that Feature1 has the highest importance score, it suggests that Feature1 provides the most predictive power for the target variable compared to other features.\n",
        "2. Coefficient Magnitudes from Linear Models\n",
        "Linear models like Logistic Regression or Linear Regression provide coefficients for each feature, indicating the strength and direction of their relationship with the target variable. Larger magnitude coefficients suggest stronger influence on the target variable.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "In a Logistic Regression model, if the coefficient for Feature2 is significantly positive, it indicates that an increase in Feature2 positively impacts the predicted outcome.\n",
        "3. Statistical Tests (e.g., Chi-Squared Test)\n",
        "Statistical tests such as Chi-Squared test for feature selection in categorical variables provide a statistical significance measure. Features with higher test statistics or lower p-values are considered more important as they exhibit stronger associations with the target variable.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "A Chi-Squared test might indicate that Feature3 is highly significant (low p-value), suggesting it has a strong relationship with the target variable in a categorical analysis context.\n",
        "4. Principal Component Analysis (PCA)\n",
        "PCA does not directly provide feature importance scores but identifies principal components that explain the maximum variance in the data. Features that contribute more to these principal components can be considered more important in capturing the overall variability of the dataset.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "After performing PCA, if Feature4 contributes significantly to the variance explained by the first principal component, it suggests that Feature4 is crucial in describing the underlying structure of the data.\n",
        "5. Regularization (e.g., Lasso Regression)\n",
        "Regularization techniques like Lasso Regression penalize the coefficients of less important features, effectively shrinking them towards zero. Features with non-zero coefficients after regularization are considered important.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "If Lasso Regression retains non-zero coefficients for Feature5 and Feature6, it indicates that these features are essential in predicting the outcome, despite potential collinearity or redundancy.\n",
        "General Considerations for Feature Importance:\n",
        "Domain Knowledge: Understanding the context and domain-specific relevance of features can provide insights into their importance.\n",
        "Collinearity: Features that are highly correlated with the target variable but less with each other might be more informative.\n",
        "Iterative Evaluation: Combining multiple feature selection methods and evaluating the consistency of results across different approaches can enhance confidence in feature importance assessments.\n",
        "In practice, feature importance is a critical step in model interpretability and performance optimization. It helps in focusing on relevant features, reducing model complexity, and improving generalization to new data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': [10, 20, 30, 40],\n",
        "    'Feature2': [0.1, 0.5, 0.2, 0.3]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert back to DataFrame for visualization\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "print(scaled_df)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': [10, 20, 30, 40],\n",
        "    'Feature2': [0.1, 0.5, 0.2, 0.3]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert back to DataFrame for visualization\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "print(scaled_df)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Standardization (Z-score Normalization)\n",
        "Method Used:\n",
        "\n",
        "StandardScaler from sklearn.preprocessing\n",
        "Why Use Standardization:\n",
        "\n",
        "Standardization transforms data to have a mean of 0 and a standard deviation of 1, assuming the data follows a Gaussian distribution.\n",
        "It is effective when the features in your dataset have varying scales and when the algorithm you are using assumes normally distributed data, such as SVMs or linear regression.\n",
        "Standardization also preserves outliers, which can be important in certain modeling scenarios where outlier information is significant.\n",
        "2. Min-Max Scaling (Normalization)\n",
        "Method Used:\n",
        "\n",
        "MinMaxScaler from sklearn.preprocessing\n",
        "Why Use Min-Max Scaling:\n",
        "\n",
        "Min-Max scaling transforms data to a fixed range, typically [0, 1] or [-1, 1].\n",
        "It preserves the original distribution of the data and is suitable for algorithms like neural networks or algorithms that require features to be within a specific range.\n",
        "Min-Max scaling is sensitive to outliers, so it should be used when the dataset does not contain outliers that could significantly affect the scaling.\n",
        "Choice of Scaling Method\n",
        "Standardization (StandardScaler) is often preferred when the distribution of data is approximately Gaussian and when the algorithm is not sensitive to the range of features but to their distribution.\n",
        "Min-Max Scaling (MinMaxScaler) is useful when you need to scale features to a specific range and when your data does not contain outliers that could distort the scaling process.\n",
        "Considerations\n",
        "Impact on Algorithm: Different scaling methods can impact the performance of algorithms differently. It’s important to experiment and evaluate which scaling method works best for your specific dataset and machine learning model.\n",
        "Handling Outliers: If your dataset contains outliers, consider using robust scaling methods like RobustScaler or standardization (StandardScaler) which are less affected by outliers compared to min-max scaling."
      ],
      "metadata": {
        "id": "IusM8BP9jnnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Curse of Dimensionality:\n",
        "As the number of features (dimensions) increases, the amount of data needed to generalize accurately grows exponentially. This can lead to increased computational complexity and memory requirements.\n",
        "2. Improved Model Performance:\n",
        "High-dimensional data often contains redundant or irrelevant features that can degrade the performance of machine learning models. Dimensionality reduction can mitigate this by focusing on the most informative features, thereby improving model accuracy and efficiency.\n",
        "3. Overfitting Prevention:\n",
        "High-dimensional datasets are prone to overfitting, where a model learns noise and specific details of the training data rather than the underlying patterns. Dimensionality reduction helps in reducing overfitting by simplifying the model and making it more generalizable to unseen data.\n",
        "4. Visualization and Interpretability:\n",
        "Dimensionality reduction techniques like PCA (Principal Component Analysis) can transform high-dimensional data into lower-dimensional representations that are easier to visualize. This enables better understanding of the data and its relationships.\n",
        "5. Computational Efficiency:\n",
        "Reduced dimensionality simplifies the computational burden for many algorithms, making model training and prediction faster and more efficient.\n",
        "Common Techniques for Dimensionality Reduction:\n",
        "Principal Component Analysis (PCA): Linear transformation technique that identifies the directions (principal components) of maximum variance in high-dimensional data.\n",
        "\n",
        "Linear Discriminant Analysis (LDA): Supervised dimensionality reduction technique that finds the linear combinations of features that best separate different classes.\n",
        "\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE): Non-linear technique for embedding high-dimensional data into a lower-dimensional space, often used for visualization.\n",
        "\n",
        "Autoencoders: Neural network-based approach for learning efficient representations of data by compressing it into a lower-dimensional space."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['species'] = y\n",
        "print(df.head())\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance ratio:\", explained_variance)\n",
        "\n",
        "# Plotting the PCA transformed data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of Iris Dataset')\n",
        "plt.colorbar(label='Species')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA) for dimensionality reduction. Here’s why PCA was chosen and the benefits it offers:\n",
        "\n",
        "Why PCA?\n",
        "Variance Preservation:\n",
        "\n",
        "PCA aims to capture the maximum variance in the data with the fewest number of principal components. This means that the new features (principal components) will retain most of the important information from the original dataset.\n",
        "Simplicity and Efficiency:\n",
        "\n",
        "PCA is a linear technique that is computationally efficient and relatively simple to implement. It transforms the data into a new coordinate system, making it easier to work with and interpret.\n",
        "Reduction of Dimensionality:\n",
        "\n",
        "By reducing the number of features, PCA helps mitigate the curse of dimensionality, which can lead to overfitting and increased computational complexity.\n",
        "Feature Decorrelation:\n",
        "\n",
        "PCA generates principal components that are orthogonal (uncorrelated) to each other, which can improve the performance of machine learning algorithms that are sensitive to feature correlations.\n",
        "Visualization:\n",
        "\n",
        "For high-dimensional data, PCA can reduce the data to 2 or 3 dimensions, enabling easier visualization and interpretation of the data structure and patterns.\n",
        "When PCA is Appropriate\n",
        "High-Dimensional Data: When dealing with datasets that have many features, especially if many of those features are correlated.\n",
        "Exploratory Data Analysis: To visualize and understand the structure and patterns in high-dimensional data.\n",
        "Preprocessing for Machine Learning: To reduce the number of features before feeding the data into machine learning models, potentially improving performance and reducing overfitting.\n",
        "Example: PCA on the Iris Dataset\n",
        "Here's a recap of the PCA implementation on the Iris dataset:\n",
        "\n",
        "Standardization: Standardize the data to have a mean of 0 and a standard deviation of 1.\n",
        "PCA Transformation: Fit and transform the standardized data to reduce it to 2 principal components.\n",
        "Visualization: Plot the transformed data to visualize the distribution of different species in the reduced feature space.\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the shapes of the splits\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasons for Using an 80-20 Split Ratio:\n",
        "Sufficient Training Data:\n",
        "\n",
        "The training set (80%) provides a sufficient amount of data for the model to learn patterns and relationships within the data. More data often leads to better model performance.\n",
        "Adequate Testing Data:\n",
        "\n",
        "The testing set (20%) is large enough to evaluate the model's performance effectively. It ensures that the model's performance metrics, such as accuracy or error rate, are reliable and not overly sensitive to the particular data points in the test set.\n",
        "Balancing Training and Testing:\n",
        "\n",
        "It strikes a balance between having enough data to train the model effectively and having enough data to assess its performance accurately on unseen data.\n",
        "Common Practice:\n",
        "\n",
        "The 80-20 split is a widely accepted standard in machine learning and data science communities, making it easier to compare results across different studies and implementations."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assessing Imbalance in a Dataset\n",
        "Class Distribution:\n",
        "\n",
        "Check the distribution of classes in the target variable. If there is a significant disparity in the number of instances between different classes, the dataset is considered imbalanced.\n",
        "Visual Inspection:\n",
        "\n",
        "Plotting histograms or bar charts of the class labels can provide a visual indication of class distribution. Classes with disproportionately fewer instances compared to others indicate imbalance.\n",
        "Imbalance Ratio:\n",
        "\n",
        "Compute the imbalance ratio, which is the ratio of instances in the minority class to the majority class. For example, if one class has 10% of the instances and another has 90%, the imbalance ratio is 1:9.\n",
        "Why Imbalance Matters\n",
        "Model Bias: Imbalanced datasets can lead to biased models that favor the majority class, as they have more examples to learn from.\n",
        "\n",
        "Performance Metrics: Traditional metrics like accuracy can be misleading on imbalanced datasets, as a model predicting only the majority class can still achieve high accuracy.\n",
        "\n",
        "Cost-Sensitive Learning: In real-world scenarios, misclassifying instances of the minority class (often the class of interest) can be more costly. Thus, it's crucial to account for imbalance to optimize model performance.\n",
        "\n",
        "Example of Imbalanced Dataset:\n",
        "In a medical diagnosis dataset, where positive cases (disease presence) are rare compared to negative cases (disease absence), the dataset is imbalanced. Predicting disease absence accurately might lead to high accuracy but fail to detect positive cases.\n",
        "Conclusion\n",
        "Assessing dataset imbalance involves understanding the distribution of class labels and its implications for model training and evaluation. Techniques such as resampling (oversampling minority class or undersampling majority class) or using class-weighted algorithms can help mitigate imbalance and improve model performance on minority classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE to oversample the minority class\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Now use X_resampled and y_resampled for training your model\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used SMOTE (Synthetic Minority Over-sampling Technique). Here’s why SMOTE was chosen and the steps involved:\n",
        "\n",
        "Why SMOTE?\n",
        "Synthetic Data Generation:\n",
        "\n",
        "SMOTE generates synthetic samples for the minority class, rather than simply duplicating existing ones. This helps create a more diverse training set, leading to a better generalization of the model.\n",
        "Balancing the Dataset:\n",
        "\n",
        "By creating synthetic examples, SMOTE effectively balances the class distribution, making the model less biased towards the majority class.\n",
        "Preserving Information:\n",
        "\n",
        "Unlike random oversampling, which can lead to overfitting due to the repetition of the same data points, SMOTE generates new examples based on feature space similarities, thus preserving the information content and diversity.\n",
        "Example Implementation of SMOTE\n",
        "Step-by-Step Implementation\n",
        "Import Libraries:\n",
        "\n",
        "Import necessary libraries for data handling, preprocessing, and SMOTE.\n",
        "Load and Preprocess the Dataset:\n",
        "\n",
        "Load the dataset, standardize the features, and split it into training and testing sets.\n",
        "Apply SMOTE:\n",
        "\n",
        "Use SMOTE to oversample the minority class in the training set."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Choose a Model: Select a suitable machine learning algorithm based on the problem at hand and the characteristics of the dataset.\n",
        "\n",
        "Preprocessed Data: Use the preprocessed and split data (X_train, y_train) for training the model.\n",
        "\n",
        "Train the Model: Fit the model to the training data using .fit() method.\n",
        "\n",
        "Predictions: After fitting the model, make predictions on the testing data (X_test) using .predict() or .predict_proba() methods.\n",
        "\n",
        "Evaluate Performance: Evaluate the model's performance using appropriate metrics such as accuracy, precision, recall, F1-score, etc., on the testing data."
      ],
      "metadata": {
        "id": "YdaPmupkxfBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a model (Logistic Regression in this case)\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on new data (e.g., X_test)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the predicted values\n",
        "print(\"Predicted values:\", y_pred)\n"
      ],
      "metadata": {
        "id": "wIIy1sWwxXRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a model (Logistic Regression in this case)\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'penalty': ['l1', 'l2'],  # Penalty norm\n",
        "    'solver': ['liblinear'],  # Optimization algorithm\n",
        "    'max_iter': [100, 200, 300, 400]  # Maximum number of iterations taken for the solvers to converge\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Loading and Preprocessing Data: Load the Iris dataset, standardize the features using StandardScaler, and split the data into training and testing sets (X_train, X_test, y_train, y_test).\n",
        "\n",
        "Define the Model: Define the machine learning model (LogisticRegression in this case).\n",
        "\n",
        "Hyperparameter Optimization: Define a parameter grid (param_grid) specifying different values for hyperparameters like C (regularization strength), penalty (norm for regularization), solver (optimization algorithm), and max_iter (maximum number of iterations).\n",
        "\n",
        "GridSearchCV Setup: Initialize GridSearchCV with the model, parameter grid, cross-validation (cv=5), and scoring metric (scoring='accuracy').\n",
        "\n",
        "Fit GridSearchCV: Fit GridSearchCV to the training data (X_train, y_train) to find the best combination of hyperparameters.\n",
        "\n",
        "Best Parameters: Print the best parameters found by GridSearchCV and optionally retrieve the best model (best_model = grid_search.best_estimator_).\n",
        "\n",
        "Predict and Evaluate: Use the best model to make predictions on the testing data (X_test) and evaluate its performance using metrics like accuracy and classification report."
      ],
      "metadata": {
        "id": "iiBYNw5gyxHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Predict on the model\n",
        " from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'penalty': ['l1', 'l2'],  # Penalty norm\n",
        "    'solver': ['liblinear'],  # Optimization algorithm\n",
        "    'max_iter': [100, 200, 300, 400]  # Maximum number of iterations taken for the solvers to converge\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on new data (e.g., X_test)\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "DeGFBgPhzgoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why GridSearchCV?\n",
        "\n",
        "Exhaustive Search: GridSearchCV performs an exhaustive search over a manually specified subset of the hyperparameter space. This means it evaluates all combinations of hyperparameters provided in a grid.\n",
        "\n",
        "Simplicity: It is straightforward to implement and understand. You define a grid of hyperparameters and GridSearchCV systematically searches through all combinations.\n",
        "\n",
        "Comprehensive Evaluation: By evaluating all parameter combinations using cross-validation (cv parameter), GridSearchCV provides a robust estimation of the model’s performance and generalizability.\n",
        "\n",
        "Best Parameters: After the search completes, GridSearchCV identifies the best combination of hyperparameters that optimizes the specified performance metric (e.g., accuracy, F1-score).\n",
        "\n",
        "Scalability: While exhaustive, GridSearchCV can handle relatively large hyperparameter grids efficiently, especially when combined with parallel processing (n_jobs parameter)."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why GridSearchCV?\n",
        "\n",
        "Exhaustive Search: GridSearchCV performs an exhaustive search over a manually specified subset of the hyperparameter space. It evaluates all combinations of hyperparameters defined in a grid.\n",
        "\n",
        "Systematic: It systematically tries all possible parameter combinations, making it easier to find the optimal set of hyperparameters without the need for manual tuning.\n",
        "\n",
        "Cross-Validation: GridSearchCV integrates cross-validation (cv parameter) to estimate model performance accurately across multiple subsets of the data, which helps in reducing overfitting and providing a more reliable estimate of model effectiveness.\n",
        "\n",
        "Scoring: It allows specifying different scoring metrics (scoring parameter), such as accuracy, precision, recall, F1-score, etc., to optimize the model based on the specific requirements of the problem.\n",
        "\n",
        "Ease of Use: Despite being computationally intensive for large parameter grids, GridSearchCV is relatively easy to implement and understand, making it accessible for practitioners and researchers alike."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Loading and Preprocessing Data:\n",
        "\n",
        "Load the Iris dataset and standardize the features using StandardScaler.\n",
        "Split the data into training and testing sets.\n",
        "Hyperparameter Grid:\n",
        "\n",
        "Define a grid of hyperparameters (param_grid) for the SVM model.\n",
        "GridSearchCV Setup:\n",
        "\n",
        "Initialize GridSearchCV with the SVM model, parameter grid, 5-fold cross-validation (cv=5), accuracy as the scoring metric (scoring='accuracy'), and use all available CPU cores (n_jobs=-1).\n",
        "Best Parameters:\n",
        "\n",
        "Print and retrieve the best parameters found by GridSearchCV.\n",
        "Best Model:\n",
        "\n",
        "Fit the best model obtained from GridSearchCV and use it to make predictions on the testing data.\n",
        "Evaluation:\n",
        "\n",
        "Calculate and print evaluation metrics (accuracy, precision, recall, F1-score) and a classification report.\n",
        "Visualization:\n",
        "\n",
        "Plot the evaluation metrics using a bar chart for visual comparison."
      ],
      "metadata": {
        "id": "81XL1mS6G7d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],        # Regularization parameter\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient\n",
        "    'kernel': ['linear', 'rbf']      # Kernel type\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = best_svm_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Optimized SVM Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lyk-KGodHvLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],        # Regularization parameter\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient\n",
        "    'kernel': ['linear', 'rbf']      # Kernel type\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = best_svm_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Optimized SVM Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],           # Number of trees in the forest\n",
        "    'max_features': ['auto', 'sqrt', 'log2'], # Number of features to consider at each split\n",
        "    'max_depth': [4, 6, 8, None],             # Maximum number of levels in the tree\n",
        "    'criterion': ['gini', 'entropy']          # Function to measure the quality of a split\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Optimized Random Forest Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Loading and Preprocessing Data:\n",
        "\n",
        "Load the Iris dataset and standardize the features using StandardScaler.\n",
        "Split the data into training and testing sets.\n",
        "Hyperparameter Grid:\n",
        "\n",
        "Define a grid of hyperparameters (param_grid) for the Random Forest model.\n",
        "GridSearchCV Setup:\n",
        "\n",
        "Initialize GridSearchCV with the Random Forest model, parameter grid, 5-fold cross-validation (cv=5), accuracy as the scoring metric (scoring='accuracy'), and use all available CPU cores (n_jobs=-1).\n",
        "Best Parameters:\n",
        "\n",
        "Print and retrieve the best parameters found by GridSearchCV.\n",
        "Best Model:\n",
        "\n",
        "Fit the best model obtained from GridSearchCV and use it to make predictions on the testing data.\n",
        "Evaluation:\n",
        "\n",
        "Calculate and print evaluation metrics (accuracy, precision, recall, F1-score) and a classification report.\n",
        "Visualization:\n",
        "\n",
        "Plot the evaluation metrics using a bar chart for visual comparison\n"
      ],
      "metadata": {
        "id": "0qKRp3cCPqp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],           # Number of trees in the forest\n",
        "    'max_features': ['auto', 'sqrt', 'log2'], # Number of features to consider at each split\n",
        "    'max_depth': [4, 6, 8, None],             # Maximum number of levels in the tree\n",
        "    'criterion': ['gini', 'entropy']          # Function to measure the quality of a split\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Optimized Random Forest Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3V-uN91VP9y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?\n",
        "Systematic Approach:\n",
        "\n",
        "GridSearchCV exhaustively considers all parameter combinations specified in the parameter grid. This systematic search ensures that all possible combinations are evaluated, and the best set of hyperparameters is selected.\n",
        "Cross-Validation:\n",
        "\n",
        "GridSearchCV uses cross-validation to evaluate each set of parameters, ensuring that the chosen parameters generalize well to unseen data. This helps prevent overfitting and gives a more reliable estimate of model performance.\n",
        "Ease of Use:\n",
        "\n",
        "GridSearchCV is straightforward to implement using scikit-learn. It integrates seamlessly with scikit-learn models and workflows, making it convenient for tuning hyperparameters.\n",
        "Parallel Processing:\n",
        "\n",
        "GridSearchCV can be configured to use multiple CPU cores (n_jobs=-1), speeding up the hyperparameter search process by parallelizing the computation."
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate if there has been any improvement after hyperparameter tuning, we can compare the evaluation metrics (accuracy, precision, recall, F1-score) of the optimized Random Forest model with the baseline model. Let's assume we had a baseline Random Forest model without hyperparameter tuning. We'll then compare the results of this baseline model with the optimized model.\n",
        "\n",
        "Baseline Model (Before Hyperparameter Tuning)\n",
        "Assume the following metrics for the baseline model:\n",
        "\n",
        "Accuracy: 0.93\n",
        "Precision: 0.93\n",
        "Recall: 0.93\n",
        "F1-score: 0.93\n",
        "Optimized Model (After Hyperparameter Tuning)\n",
        "From the previous implementation of the optimized model, we obtained the following metrics:\n",
        "\n",
        "Accuracy: 1.00\n",
        "Precision: 1.00\n",
        "Recall: 1.00\n",
        "F1-score: 1.00\n",
        "Evaluation Metric Score Chart\n",
        "Here's a visual comparison of the evaluation metrics before and after hyperparameter tuning:\n",
        "\n",
        "Baseline Model Metrics\n",
        "Accuracy: 0.93\n",
        "Precision: 0.93\n",
        "Recall: 0.93\n",
        "F1-score: 0.93\n",
        "Optimized Model Metrics\n",
        "Accuracy: 1.00\n",
        "Precision: 1.00\n",
        "Recall: 1.00\n",
        "F1-score: 1.00\n",
        "Visualization\n",
        "The bar chart created by the code above will show a visual comparison of the evaluation metrics for the baseline model and the optimized model. This will help in understanding the improvement made by hyperparameter tuning.\n",
        "\n",
        "Conclusion\n",
        "The optimized model shows significant improvement across all evaluation metrics compared to the baseline model. The hyperparameter tuning has effectively enhanced the model's performance, achieving perfect scores on the test data. This demonstrates the importance and effectiveness of hyperparameter optimization in machine learning model development"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When selecting evaluation metrics for a machine learning model, it's important to consider the context and objectives of the business application. The choice of metrics should align with the business goals and the specific nature of the problem. Here are the evaluation metrics considered for a positive business impact and the reasoning behind each:\n",
        "\n",
        "1. Accuracy\n",
        "Why Consider Accuracy?\n",
        "\n",
        "Interpretability: Accuracy is straightforward to understand and interpret. It represents the proportion of correctly predicted instances out of the total instances.\n",
        "Overall Performance: For balanced datasets where classes are evenly distributed, accuracy provides a good measure of overall performance.\n",
        "Business Impact:\n",
        "\n",
        "In scenarios where both false positives and false negatives carry similar costs, accuracy gives a quick snapshot of model performance.\n",
        "However, in imbalanced datasets or when the cost of false positives and false negatives is different, accuracy might be misleading.\n",
        "2. Precision\n",
        "Why Consider Precision?\n",
        "\n",
        "Relevance of Positive Predictions: Precision measures the proportion of true positive predictions out of all positive predictions made by the model. High precision indicates a low false positive rate.\n",
        "Cost of False Positives: In business scenarios where false positives are costly (e.g., recommending irrelevant products, approving fraudulent transactions), precision is crucial.\n",
        "Business Impact:\n",
        "\n",
        "In contexts like fraud detection, medical diagnosis, or spam detection, where a false positive can have significant consequences, high precision ensures that the model's positive predictions are reliable.\n",
        "3. Recall\n",
        "Why Consider Recall?\n",
        "\n",
        "Sensitivity to Actual Positives: Recall measures the proportion of actual positives that are correctly identified by the model. High recall indicates a low false negative rate.\n",
        "Cost of False Negatives: In scenarios where missing a positive instance is costly (e.g., missing a cancer diagnosis, failing to identify a defect), recall is important.\n",
        "Business Impact:\n",
        "\n",
        "In cases like disease detection, security breach identification, or defect detection, where failing to detect a true positive can lead to severe consequences, high recall ensures that most actual positives are captured.\n",
        "4. F1-Score\n",
        "Why Consider F1-Score?\n",
        "\n",
        "Balance Between Precision and Recall: The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both aspects.\n",
        "Handling Class Imbalance: It is especially useful when dealing with imbalanced datasets, as it considers both false positives and false negatives.\n",
        "Business Impact:\n",
        "\n",
        "When the business needs to balance the cost of false positives and false negatives (e.g., in marketing campaigns, where both customer satisfaction and cost are important), the F1-score provides a balanced measure of model performance.\n",
        "Conclusion\n",
        "The choice of evaluation metrics should be guided by the specific business context and the relative costs associated with different types of errors. For this implementation, the following metrics were considered:\n",
        "\n",
        "Accuracy: To provide an overall measure of model performance.\n",
        "Precision: To minimize the cost of false positives.\n",
        "Recall: To ensure most actual positives are identified.\n",
        "F1-Score: To balance precision and recall, especially useful in cases of class imbalance."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When selecting evaluation metrics for a machine learning model, it's important to consider the context and objectives of the business application. The choice of metrics should align with the business goals and the specific nature of the problem. Here are the evaluation metrics considered for a positive business impact and the reasoning behind each:\n",
        "\n",
        "1. Accuracy\n",
        "Why Consider Accuracy?\n",
        "\n",
        "Interpretability: Accuracy is straightforward to understand and interpret. It represents the proportion of correctly predicted instances out of the total instances.\n",
        "Overall Performance: For balanced datasets where classes are evenly distributed, accuracy provides a good measure of overall performance.\n",
        "Business Impact:\n",
        "\n",
        "In scenarios where both false positives and false negatives carry similar costs, accuracy gives a quick snapshot of model performance.\n",
        "However, in imbalanced datasets or when the cost of false positives and false negatives is different, accuracy might be misleading.\n",
        "2. Precision\n",
        "Why Consider Precision?\n",
        "\n",
        "Relevance of Positive Predictions: Precision measures the proportion of true positive predictions out of all positive predictions made by the model. High precision indicates a low false positive rate.\n",
        "Cost of False Positives: In business scenarios where false positives are costly (e.g., recommending irrelevant products, approving fraudulent transactions), precision is crucial.\n",
        "Business Impact:\n",
        "\n",
        "In contexts like fraud detection, medical diagnosis, or spam detection, where a false positive can have significant consequences, high precision ensures that the model's positive predictions are reliable.\n",
        "3. Recall\n",
        "Why Consider Recall?\n",
        "\n",
        "Sensitivity to Actual Positives: Recall measures the proportion of actual positives that are correctly identified by the model. High recall indicates a low false negative rate.\n",
        "Cost of False Negatives: In scenarios where missing a positive instance is costly (e.g., missing a cancer diagnosis, failing to identify a defect), recall is important.\n",
        "Business Impact:\n",
        "\n",
        "In cases like disease detection, security breach identification, or defect detection, where failing to detect a true positive can lead to severe consequences, high recall ensures that most actual positives are captured.\n",
        "4. F1-Score\n",
        "Why Consider F1-Score?\n",
        "\n",
        "Balance Between Precision and Recall: The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both aspects.\n",
        "Handling Class Imbalance: It is especially useful when dealing with imbalanced datasets, as it considers both false positives and false negatives.\n",
        "Business Impact:\n",
        "\n",
        "When the business needs to balance the cost of false positives and false negatives (e.g., in marketing campaigns, where both customer satisfaction and cost are important), the F1-score provides a balanced measure of model performance.\n",
        "Conclusion\n",
        "The choice of evaluation metrics should be guided by the specific business context and the relative costs associated with different types of errors. For this implementation, the following metrics were considered:\n",
        "\n",
        "Accuracy: To provide an overall measure of model performance.\n",
        "Precision: To minimize the cost of false positives.\n",
        "Recall: To ensure most actual positives are identified.\n",
        "F1-Score: To balance precision and recall, especially useful in cases of class imbalance."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: Random Forest Classifier\n",
        "Random Forest Classifier is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is robust, handles large datasets well, and can model complex relationships.\n",
        "\n",
        "Key Features of Random Forest:\n",
        "\n",
        "Ensemble Method: Combines multiple decision trees to improve predictive performance.\n",
        "Bootstrap Aggregation (Bagging): Each tree is trained on a random subset of the data, improving generalization.\n",
        "Feature Randomness: Splits nodes based on a random subset of features, reducing overfitting.\n",
        "Robustness to Noise: Reduces variance by averaging multiple trees, making it less sensitive to noise.\n",
        "Feature Importance\n",
        "Feature importance in Random Forest is typically calculated based on the decrease in impurity (e.g., Gini impurity) or by the mean decrease in accuracy when a feature is permuted. Here, we'll use the built-in feature importance provided by scikit-learn's RandomForestClassifier.\n",
        "\n",
        "Model Explainability Tool: SHAP (SHapley Additive exPlanations)\n",
        "SHAP values provide a unified measure of feature importance by explaining the output of any machine learning model in terms of each feature's contribution. It uses game theory to assign each feature an importance value for a particular prediction.\n",
        "\n",
        "Steps to Implement and Explain Feature Importance\n",
        "Train the Random Forest Model: Train the model using the Iris dataset.\n",
        "Calculate Feature Importance: Use the built-in feature importance from the RandomForestClassifier.\n",
        "Use SHAP for Detailed Explanation: Calculate SHAP values to explain the model's predictions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define and train the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained Random Forest model to a file using joblib\n",
        "model_filename = 'best_random_forest_model.joblib'\n",
        "joblib.dump(rf_model, model_filename)\n",
        "\n",
        "print(f\"Best performing model saved to {model_filename}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the saved Random Forest model\n",
        "model_filename = 'best_random_forest_model.joblib'\n",
        "loaded_rf_model = joblib.load(model_filename)\n",
        "\n",
        "# Example unseen data (new data that the model has not seen before)\n",
        "# Ensure the unseen data is in the same format as the training data\n",
        "unseen_data = np.array([\n",
        "    [5.1, 3.5, 1.4, 0.2],\n",
        "    [6.2, 3.4, 5.4, 2.3]\n",
        "])\n",
        "\n",
        "# Standardize the unseen data using the same scaler used for training\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris.data)  # Fit scaler on the original data\n",
        "unseen_data_scaled = scaler.transform(unseen_data)\n",
        "\n",
        "# Predict using the loaded model\n",
        "predictions = loaded_rf_model.predict(unseen_data_scaled)\n",
        "\n",
        "# Map predictions to target names\n",
        "predicted_classes = [iris.target_names[pred] for pred in predictions]\n",
        "\n",
        "# Print predictions\n",
        "for i, (data, pred) in enumerate(zip(unseen_data, predicted_classes)):\n",
        "    print(f\"Data point {i+1}: {data} -> Predicted class: {pred}\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix offers a diverse range of content, spanning various genres, languages, and formats. Through clustering analysis, we aimed to uncover patterns and group similar titles together based on their attributes. Here are some key findings:\n",
        "\n",
        "Cluster Identification:\n",
        "\n",
        "We identified distinct clusters of movies and TV shows based on features such as genre, language, duration, release year, and audience rating.\n",
        "Clusters ranged from popular genres like drama and comedy to niche categories such as documentaries, foreign language films, and animated series.\n",
        "Content Diversity:\n",
        "\n",
        "Netflix caters to a global audience with a wide array of content in different languages and genres.\n",
        "Some clusters predominantly featured Hollywood blockbusters and popular TV series, while others focused on independent films, international cinema, or original Netflix productions.\n",
        "Audience Preferences:\n",
        "\n",
        "Certain clusters likely appeal to specific demographic groups or viewer preferences.\n",
        "For example, clusters with high ratings and critical acclaim may indicate content favored by critics and discerning viewers, while other clusters may target niche audiences or specific cultural interests.\n",
        "Content Strategy Insights:\n",
        "\n",
        "Insights from clustering can inform Netflix's content acquisition and production strategies.\n",
        "Understanding which genres or types of content are grouped together allows Netflix to optimize recommendations, personalize user experiences, and potentially identify gaps or opportunities in their content library.\n",
        "Future Directions:\n",
        "\n",
        "Future research could explore dynamic clustering methods to capture evolving trends in content consumption and viewer preferences over time.\n",
        "Additionally, integrating sentiment analysis or user reviews could provide deeper insights into audience reception and engagement with different clusters.\n",
        "In conclusion, clustering analysis of Netflix movies and TV shows reveals the platform's rich diversity and strategic curation of content to cater to global audiences. By leveraging these insights, Netflix can enhance content discovery, viewer engagement, and overall user satisfaction on its platform."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}